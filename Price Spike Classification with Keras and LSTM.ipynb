{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras import Input\n",
    "from keras.engine import Model\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Concatenate, concatenate\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# features is a list of strings of feature names \n",
    "\n",
    "def build_model(features, data_length):\n",
    "    \n",
    "    inputs_list = [] \n",
    "    for feature_name in features:\n",
    "        inputs_list.append((Input(shape=(data_length,1), name=feature_name)))\n",
    "    \n",
    "    layers = [] \n",
    "    for i, input_name in enumerate(inputs_list): \n",
    "        layers.append(LSTM(64, return_sequences=False,dropout=0.2)(inputs_list[i]) )\n",
    "        \n",
    "    output = concatenate(layers) \n",
    "    output = Dense(3, activation='softmax', name='IsSpike')(output)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs = inputs_list,\n",
    "        outputs = [output]\n",
    "    )\n",
    "    \n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    \n",
    "    return model    \n",
    "\n",
    "data_length = 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Shape\n",
    "\n",
    "* Price  ----------> LSTM --\\\n",
    "* Google Trends ---> LSTM ---> Dense Layer -> Softmax -> Output: Is Spike (1,0,-1) \n",
    "* Volume ----------> LSTM --/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* Input: Price, Google Trends, and Volume for time t0-t9 (10 hours of data) \n",
    "* Output: Is Spike (1,0,-1) for t10 \n",
    "    * Using 10 hours (t0-t9) of Price, Google Trends, and Volume to predict the price movement at t11 (t10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shoya\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Shoya\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Shoya\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Volume_BTC</th>\n",
       "      <th>Bitcoin_Adj</th>\n",
       "      <th>Close</th>\n",
       "      <th>Price_lagged</th>\n",
       "      <th>Is Spike</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.776791</td>\n",
       "      <td>0.471970</td>\n",
       "      <td>0.484557</td>\n",
       "      <td>0.484557</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.463316</td>\n",
       "      <td>0.439996</td>\n",
       "      <td>0.538331</td>\n",
       "      <td>0.538331</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.725079</td>\n",
       "      <td>0.529463</td>\n",
       "      <td>0.520715</td>\n",
       "      <td>0.520715</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.210661</td>\n",
       "      <td>0.416611</td>\n",
       "      <td>0.566098</td>\n",
       "      <td>0.566098</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.594148</td>\n",
       "      <td>0.445509</td>\n",
       "      <td>0.568881</td>\n",
       "      <td>0.568881</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Volume_BTC  Bitcoin_Adj     Close  Price_lagged  Is Spike\n",
       "1    0.776791     0.471970  0.484557      0.484557       1.0\n",
       "2    0.463316     0.439996  0.538331      0.538331       1.0\n",
       "3    0.725079     0.529463  0.520715      0.520715      -1.0\n",
       "4    0.210661     0.416611  0.566098      0.566098       0.0\n",
       "5    0.594148     0.445509  0.568881      0.568881      -1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Volume_BTC</th>\n",
       "      <th>Bitcoin_Adj</th>\n",
       "      <th>Close</th>\n",
       "      <th>Price_lagged</th>\n",
       "      <th>Is Spike</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30285</th>\n",
       "      <td>0.455905</td>\n",
       "      <td>0.449846</td>\n",
       "      <td>0.546519</td>\n",
       "      <td>0.546519</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30286</th>\n",
       "      <td>0.308749</td>\n",
       "      <td>0.457405</td>\n",
       "      <td>0.549938</td>\n",
       "      <td>0.549938</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30287</th>\n",
       "      <td>0.632915</td>\n",
       "      <td>0.453625</td>\n",
       "      <td>0.563428</td>\n",
       "      <td>0.563428</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30288</th>\n",
       "      <td>0.629822</td>\n",
       "      <td>0.434124</td>\n",
       "      <td>0.506657</td>\n",
       "      <td>0.506657</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30289</th>\n",
       "      <td>0.564642</td>\n",
       "      <td>0.445374</td>\n",
       "      <td>0.493535</td>\n",
       "      <td>0.493535</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Volume_BTC  Bitcoin_Adj     Close  Price_lagged  Is Spike\n",
       "30285    0.455905     0.449846  0.546519      0.546519       1.0\n",
       "30286    0.308749     0.457405  0.549938      0.549938      -1.0\n",
       "30287    0.632915     0.453625  0.563428      0.563428       0.0\n",
       "30288    0.629822     0.434124  0.506657      0.506657       0.0\n",
       "30289    0.564642     0.445374  0.493535      0.493535      -1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "master_df = pd.read_csv('C:/Users/Shoya/surf/data/master_df_v3.csv', encoding='latin1')\n",
    "df = master_df[['Timestamp', 'Close', 'Volume_(BTC)', 'Volume_(Currency)', 'Date(UTC)', 'Bitcoin (Adj.Overlap)', \n",
    "               'Close Price % Change', 'Close Price % Change (Abs)', 'Is Spike']]\n",
    "\n",
    "# lag inputs depending on data_length \n",
    "df['Price_lagged'] = df['Close']#.shift(data_length)\n",
    "df['Volume_BTC'] = df['Volume_(BTC)']#.shift(data_length)\n",
    "df['Bitcoin_Adj'] = df['Bitcoin (Adj.Overlap)']#.shift(data_length)\n",
    "\n",
    "df = df.dropna()\n",
    "cols = ['Volume_BTC','Bitcoin_Adj', 'Close', 'Price_lagged']\n",
    "\n",
    "# Stationalize Data by taking log differences\n",
    "data_array = np.diff(np.log(df[cols]), axis=0)\n",
    "\n",
    "# Min-Max Scale \n",
    "\n",
    "scalers = {}\n",
    "datas = [] \n",
    "\n",
    "df_scaled = pd.DataFrame(columns=cols)\n",
    "\n",
    "\n",
    "############################################################\n",
    "#  Fix below - I am scaling the whole data set together, when I should scale the train and test datasets separately\n",
    "############################################################\n",
    "\n",
    "for i in range(len(cols)): \n",
    "    scalers[cols[i]] = MinMaxScaler()\n",
    "    #print('data', data_array[:,i])\n",
    "    \n",
    "    col_data = data_array[:,i]\n",
    "    col_data = np.reshape(col_data, (len(col_data), 1))\n",
    "    \n",
    "    data = scalers[cols[i]].fit_transform( col_data )\n",
    "    #print('scaled', data)\n",
    "    data = np.reshape(data, (1, len(data)))\n",
    "    df_scaled[cols[i]] = data[0]\n",
    "    \n",
    "df_scaled['Is Spike'] = df['Is Spike']\n",
    "df_scaled.dropna(inplace=True)\n",
    "display(df_scaled.head())\n",
    "display(df_scaled.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0., -1., ...,  1.,  1., -1.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# split and reshape data to feed into RNN\n",
    "\n",
    "# X_timestamp = df_scaled['Timestamp'].values\n",
    "X_volume = df_scaled['Volume_BTC'].values\n",
    "X_trends = df_scaled['Bitcoin_Adj'].values\n",
    "X_lagged_price = df_scaled['Price_lagged'].values\n",
    "\n",
    "Y_is_spike = df_scaled['Is Spike'].values \n",
    "\n",
    "train_size = int(len(X_volume) * 0.85)\n",
    "train_size = int(train_size/data_length) * data_length\n",
    "\n",
    "test_size_index = int(len(X_volume)/data_length)*data_length\n",
    "\n",
    "X_train_volume = []\n",
    "X_test_volume = [] \n",
    "X_train_trends = []\n",
    "X_test_trends = []\n",
    "X_train_lagged_price = []\n",
    "X_test_lagged_price = []\n",
    "Y_train_is_spike = [] \n",
    "Y_test_is_spike = [] \n",
    "\n",
    "for i in range(train_size-data_length):\n",
    "    vol_temp = []\n",
    "    trends_temp = []\n",
    "    price_temp = []\n",
    "    for j in range(data_length):\n",
    "        vol_temp.append(X_volume[i+j])\n",
    "        trends_temp.append(X_trends[i+j])\n",
    "        price_temp.append(X_lagged_price[i+j])\n",
    "    X_train_volume.append(vol_temp)\n",
    "    X_train_trends.append(trends_temp)\n",
    "    X_train_lagged_price.append(price_temp)\n",
    "    \n",
    "    Y_train_is_spike.append(Y_is_spike[i+data_length])\n",
    "\n",
    "for i in range(test_size_index-train_size-data_length):\n",
    "    vol_temp = []\n",
    "    trends_temp = [] \n",
    "    price_temp = [] \n",
    "    for j in range(data_length):\n",
    "        vol_temp.append(X_volume[train_size+i+j])\n",
    "        trends_temp.append(X_trends[train_size+i+j])\n",
    "        price_temp.append(X_lagged_price[train_size+i+j])\n",
    "    X_test_volume.append(vol_temp)\n",
    "    X_test_trends.append(trends_temp)\n",
    "    X_test_lagged_price.append(price_temp)\n",
    "    \n",
    "    Y_test_is_spike.append(Y_is_spike[train_size+i+data_length])\n",
    "    \n",
    "X_train_volume = np.array(X_train_volume)\n",
    "X_test_volume =  np.array(X_test_volume)\n",
    "X_train_trends = np.array(X_train_trends)\n",
    "X_test_trends = np.array(X_test_trends)\n",
    "X_train_lagged_price = np.array(X_train_lagged_price)\n",
    "X_test_lagged_price = np.array(X_test_lagged_price)\n",
    "Y_train_is_spike =  np.array(Y_train_is_spike)\n",
    "Y_test_is_spike = np.array(Y_test_is_spike)\n",
    "    \n",
    "    \n",
    "Y_train_is_spike_onehot = to_categorical(Y_train_is_spike, num_classes=3)\n",
    "Y_test_is_spike_onehot = to_categorical(Y_test_is_spike,num_classes=3)\n",
    "display(Y_train_is_spike)\n",
    "\n",
    "# y = pd.DataFrame(Y_train_is_spike_onehot)\n",
    "# y['actual'] = Y_train_is_spike\n",
    "# display(y.head(25))\n",
    "    \n",
    "# display(X_train_trends.shape)\n",
    "# display(Y_train_is_spike.shape)\n",
    "\n",
    "#display(X_train_lagged_price)\n",
    "#display(Y_train_is_spike)\n",
    "\n",
    "# df_train = pd.DataFrame(X_train_lagged_price)\n",
    "# df_train['label'] = Y_train_is_spike\n",
    "# display(df_train.tail(20))\n",
    "# display(df_scaled.head(30))\n",
    "# display(df_train.head(30))\n",
    "\n",
    "#--------------------------------\n",
    "\n",
    "# # X_train_timestamp, X_test_timestamp = X_timestamp[:train_size], X_timestamp[train_size:test_size_index ]\n",
    "# X_train_volume, X_test_volume = X_volume[:train_size], X_volume[train_size:test_size_index ]\n",
    "# X_train_trends, X_test_trends = X_trends[:train_size], X_trends[train_size:test_size_index ]\n",
    "# X_train_lagged_price, X_test_lagged_price = X_lagged_price[:train_size], X_lagged_price[train_size:test_size_index ]\n",
    "\n",
    "# # becasue I lagged the x inputs, I should forward the Y's by the data_length as well \n",
    "# Y_train_is_spike, Y_test_is_spike = Y_is_spike[data_length:train_size], Y_is_spike[train_size+data_length:test_size_index ]\n",
    "\n",
    "\n",
    "# # X.shape is (samples, timesteps, dimension) \n",
    "# # timestemps is 15, samples is just however many nobs there are (but it doesn't matter, so it should be None)\n",
    "\n",
    "\n",
    "X_train_volume = np.reshape(X_train_volume, (X_train_volume.shape[0],data_length,1) ) \n",
    "X_train_trends = np.reshape(X_train_trends, (X_train_trends.shape[0],data_length,1) ) \n",
    "X_train_lagged_price = np.reshape(X_train_lagged_price, (X_train_lagged_price.shape[0], data_length, 1))\n",
    "\n",
    "X_test_volume = np.reshape(X_test_volume, (X_test_volume.shape[0],data_length,1) ) \n",
    "X_test_trends = np.reshape(X_test_trends, (X_test_trends.shape[0],data_length,1) )  \n",
    "X_test_lagged_price = np.reshape(X_test_lagged_price, (X_test_lagged_price.shape[0],data_length,1))\n",
    "\n",
    "\n",
    "# # X_train_timestamp = np.reshape(X_train_timestamp, (int(X_train_timestamp.shape[0]/data_length),data_length,1) ) \n",
    "# X_train_volume = np.reshape(X_train_volume, (int(X_train_volume.shape[0]/data_length),data_length,1) ) \n",
    "# X_train_trends = np.reshape(X_train_trends, (int(X_train_trends.shape[0]/data_length),data_length,1) ) \n",
    "# X_train_lagged_price = np.reshape(X_train_lagged_price, (int(X_train_lagged_price.shape[0]/data_length), data_length, 1))\n",
    "\n",
    "# # X_test_timestamp = np.reshape(X_test_timestamp, (int(X_test_timestamp.shape[0]/data_length),data_length,1) ) \n",
    "# X_test_volume = np.reshape(X_test_volume, (int(X_test_volume.shape[0]/data_length),data_length,1) ) \n",
    "# X_test_trends = np.reshape(X_test_trends, (int(X_test_trends.shape[0]/data_length),data_length,1) )  \n",
    "# X_test_lagged_price = np.reshape(X_test_lagged_price, (int(X_test_lagged_price.shape[0]/data_length),data_length,1))\n",
    "\n",
    "\n",
    "# # Don't need the 1 for the third dimension for Y's??\n",
    "\n",
    "\n",
    "# Y_train_is_spike = np.reshape(Y_train_is_spike, (int(Y_train_is_spike.shape[0]/data_length),  data_length) ) \n",
    "# Y_test_is_spike = np.reshape(Y_test_is_spike, (int(Y_test_is_spike.shape[0]/data_length),  data_length) )\n",
    "\n",
    "#-----------------------------------\n",
    "\n",
    "\n",
    "# instead of using input 1,2,3,4,5,6,7,8,9,10 to predict output for 11,12,13,14,15,16,17,18,19,20\n",
    "# I want to use input 1,2,3,4,5,6,7,8,9,10 to predict output for 11, then 2,3,4,5,6,7,8,9,10,11 to predict output for 12 \n",
    "\n",
    "# right now I am actually feeding input 1,2,3,4,5,6,7,8,9,10 to predict output for 1,2,3,4,5,6,7,8,9,10. \n",
    "# instead I should at least feed 1,2,3..8,9,10 to predict 11,12,13,14,15,16,17,18,19,20 -> lag everything by data_length! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "25728/25728 [==============================] - 18s - loss: 1.0825 - categorical_accuracy: 0.4242    \n",
      "Epoch 2/400\n",
      "25728/25728 [==============================] - 18s - loss: 1.0816 - categorical_accuracy: 0.4242    \n",
      "Epoch 3/400\n",
      "25728/25728 [==============================] - 19s - loss: 1.0814 - categorical_accuracy: 0.4242    \n",
      "Epoch 4/400\n",
      "25728/25728 [==============================] - 18s - loss: 1.0803 - categorical_accuracy: 0.4242    \n",
      "Epoch 5/400\n",
      "25728/25728 [==============================] - 18s - loss: 1.0783 - categorical_accuracy: 0.4245    \n",
      "Epoch 6/400\n",
      "25728/25728 [==============================] - 18s - loss: 1.0712 - categorical_accuracy: 0.4328    \n",
      "Epoch 7/400\n",
      "25728/25728 [==============================] - 20s - loss: 1.0650 - categorical_accuracy: 0.4476    \n",
      "Epoch 8/400\n",
      "25728/25728 [==============================] - 20s - loss: 1.0591 - categorical_accuracy: 0.4614    \n",
      "Epoch 9/400\n",
      "25728/25728 [==============================] - 19s - loss: 1.0550 - categorical_accuracy: 0.4698    \n",
      "Epoch 10/400\n",
      "25728/25728 [==============================] - 16s - loss: 1.0499 - categorical_accuracy: 0.4780    \n",
      "Epoch 11/400\n",
      "25728/25728 [==============================] - 17s - loss: 1.0419 - categorical_accuracy: 0.4877    \n",
      "Epoch 12/400\n",
      "25728/25728 [==============================] - 15s - loss: 1.0298 - categorical_accuracy: 0.4981    \n",
      "Epoch 13/400\n",
      "25728/25728 [==============================] - 14s - loss: 1.0182 - categorical_accuracy: 0.5044    \n",
      "Epoch 14/400\n",
      "25728/25728 [==============================] - 14s - loss: 1.0058 - categorical_accuracy: 0.5129    \n",
      "Epoch 15/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.9979 - categorical_accuracy: 0.5190    \n",
      "Epoch 16/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.9914 - categorical_accuracy: 0.5251    \n",
      "Epoch 17/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.9810 - categorical_accuracy: 0.5323    \n",
      "Epoch 18/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.9723 - categorical_accuracy: 0.5399    \n",
      "Epoch 19/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.9620 - categorical_accuracy: 0.5470    \n",
      "Epoch 20/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.9457 - categorical_accuracy: 0.5616    \n",
      "Epoch 21/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.9322 - categorical_accuracy: 0.5708    \n",
      "Epoch 22/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.9202 - categorical_accuracy: 0.5797    \n",
      "Epoch 23/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.9103 - categorical_accuracy: 0.5890    \n",
      "Epoch 24/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8993 - categorical_accuracy: 0.5955    \n",
      "Epoch 25/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8946 - categorical_accuracy: 0.5934    \n",
      "Epoch 26/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8937 - categorical_accuracy: 0.5950    \n",
      "Epoch 27/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8868 - categorical_accuracy: 0.5989    \n",
      "Epoch 28/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.8844 - categorical_accuracy: 0.5995    \n",
      "Epoch 29/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8758 - categorical_accuracy: 0.6056    \n",
      "Epoch 30/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8754 - categorical_accuracy: 0.6047    \n",
      "Epoch 31/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8755 - categorical_accuracy: 0.6021    \n",
      "Epoch 32/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8684 - categorical_accuracy: 0.6070    \n",
      "Epoch 33/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8618 - categorical_accuracy: 0.6112    \n",
      "Epoch 34/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8589 - categorical_accuracy: 0.6129    \n",
      "Epoch 35/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8562 - categorical_accuracy: 0.6126    \n",
      "Epoch 36/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8509 - categorical_accuracy: 0.6165    \n",
      "Epoch 37/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8476 - categorical_accuracy: 0.6149    \n",
      "Epoch 38/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8465 - categorical_accuracy: 0.6163    \n",
      "Epoch 39/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8393 - categorical_accuracy: 0.6177    \n",
      "Epoch 40/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8372 - categorical_accuracy: 0.6219    \n",
      "Epoch 41/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8334 - categorical_accuracy: 0.6211    \n",
      "Epoch 42/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8307 - categorical_accuracy: 0.6231    \n",
      "Epoch 43/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8323 - categorical_accuracy: 0.6215    \n",
      "Epoch 44/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8272 - categorical_accuracy: 0.6243    \n",
      "Epoch 45/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8216 - categorical_accuracy: 0.6251    \n",
      "Epoch 46/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8240 - categorical_accuracy: 0.6276    \n",
      "Epoch 47/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8181 - categorical_accuracy: 0.6310    \n",
      "Epoch 48/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8173 - categorical_accuracy: 0.6294    \n",
      "Epoch 49/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8172 - categorical_accuracy: 0.6277    \n",
      "Epoch 50/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8165 - categorical_accuracy: 0.6285    \n",
      "Epoch 51/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8102 - categorical_accuracy: 0.6322    \n",
      "Epoch 52/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8117 - categorical_accuracy: 0.6314    \n",
      "Epoch 53/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8083 - categorical_accuracy: 0.6337    \n",
      "Epoch 54/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8051 - categorical_accuracy: 0.6329    \n",
      "Epoch 55/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8012 - categorical_accuracy: 0.6362    \n",
      "Epoch 56/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8028 - categorical_accuracy: 0.6317    \n",
      "Epoch 57/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.8004 - categorical_accuracy: 0.6356    \n",
      "Epoch 58/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7966 - categorical_accuracy: 0.6350    \n",
      "Epoch 59/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7962 - categorical_accuracy: 0.6338    \n",
      "Epoch 60/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7915 - categorical_accuracy: 0.6381    \n",
      "Epoch 61/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7878 - categorical_accuracy: 0.6401    \n",
      "Epoch 62/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7858 - categorical_accuracy: 0.6383    \n",
      "Epoch 63/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7884 - categorical_accuracy: 0.6380    \n",
      "Epoch 64/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7832 - categorical_accuracy: 0.6391    \n",
      "Epoch 65/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7824 - categorical_accuracy: 0.6384    \n",
      "Epoch 66/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7799 - categorical_accuracy: 0.6401    \n",
      "Epoch 67/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7839 - categorical_accuracy: 0.6378    \n",
      "Epoch 68/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7776 - categorical_accuracy: 0.6405    \n",
      "Epoch 69/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7735 - categorical_accuracy: 0.6438    \n",
      "Epoch 70/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7724 - categorical_accuracy: 0.6425    \n",
      "Epoch 71/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7668 - categorical_accuracy: 0.6440    \n",
      "Epoch 72/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25728/25728 [==============================] - 14s - loss: 0.7707 - categorical_accuracy: 0.6440    \n",
      "Epoch 73/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7701 - categorical_accuracy: 0.6433    \n",
      "Epoch 74/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7677 - categorical_accuracy: 0.6457    \n",
      "Epoch 75/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7697 - categorical_accuracy: 0.6444    \n",
      "Epoch 76/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7665 - categorical_accuracy: 0.6468    \n",
      "Epoch 77/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7642 - categorical_accuracy: 0.6461    \n",
      "Epoch 78/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7626 - categorical_accuracy: 0.6440    \n",
      "Epoch 79/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7619 - categorical_accuracy: 0.6480    \n",
      "Epoch 80/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7602 - categorical_accuracy: 0.6470    \n",
      "Epoch 81/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7594 - categorical_accuracy: 0.6467    \n",
      "Epoch 82/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7539 - categorical_accuracy: 0.6507    \n",
      "Epoch 83/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7550 - categorical_accuracy: 0.6524    \n",
      "Epoch 84/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7569 - categorical_accuracy: 0.6461    \n",
      "Epoch 85/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7557 - categorical_accuracy: 0.6475    \n",
      "Epoch 86/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7499 - categorical_accuracy: 0.6488    \n",
      "Epoch 87/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7553 - categorical_accuracy: 0.6453    \n",
      "Epoch 88/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7524 - categorical_accuracy: 0.6479    \n",
      "Epoch 89/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7560 - categorical_accuracy: 0.6461    \n",
      "Epoch 90/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7533 - categorical_accuracy: 0.6479    \n",
      "Epoch 91/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7497 - categorical_accuracy: 0.6490    \n",
      "Epoch 92/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7481 - categorical_accuracy: 0.6521    \n",
      "Epoch 93/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7464 - categorical_accuracy: 0.6512    \n",
      "Epoch 94/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7434 - categorical_accuracy: 0.6532    \n",
      "Epoch 95/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7480 - categorical_accuracy: 0.6527    \n",
      "Epoch 96/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7453 - categorical_accuracy: 0.6524    \n",
      "Epoch 97/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7460 - categorical_accuracy: 0.6506    \n",
      "Epoch 98/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7404 - categorical_accuracy: 0.6511    \n",
      "Epoch 99/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7419 - categorical_accuracy: 0.6517    \n",
      "Epoch 100/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7398 - categorical_accuracy: 0.6546    \n",
      "Epoch 101/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7404 - categorical_accuracy: 0.6517    \n",
      "Epoch 102/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7419 - categorical_accuracy: 0.6531    \n",
      "Epoch 103/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7406 - categorical_accuracy: 0.6545    \n",
      "Epoch 104/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7432 - categorical_accuracy: 0.6516    \n",
      "Epoch 105/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7381 - categorical_accuracy: 0.6525    \n",
      "Epoch 106/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7382 - categorical_accuracy: 0.6567    \n",
      "Epoch 107/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7398 - categorical_accuracy: 0.6501    \n",
      "Epoch 108/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7363 - categorical_accuracy: 0.6526    \n",
      "Epoch 109/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7368 - categorical_accuracy: 0.6571    \n",
      "Epoch 110/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7352 - categorical_accuracy: 0.6566    \n",
      "Epoch 111/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7342 - categorical_accuracy: 0.6545    \n",
      "Epoch 112/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7344 - categorical_accuracy: 0.6533    \n",
      "Epoch 113/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7378 - categorical_accuracy: 0.6507    \n",
      "Epoch 114/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7296 - categorical_accuracy: 0.6569    \n",
      "Epoch 115/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7342 - categorical_accuracy: 0.6556    \n",
      "Epoch 116/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7305 - categorical_accuracy: 0.6602    \n",
      "Epoch 117/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7299 - categorical_accuracy: 0.6550    \n",
      "Epoch 118/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7315 - categorical_accuracy: 0.6576    \n",
      "Epoch 119/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7340 - categorical_accuracy: 0.6509    \n",
      "Epoch 120/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7296 - categorical_accuracy: 0.6558    \n",
      "Epoch 121/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7252 - categorical_accuracy: 0.6587    \n",
      "Epoch 122/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7306 - categorical_accuracy: 0.6573    \n",
      "Epoch 123/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7281 - categorical_accuracy: 0.6582    \n",
      "Epoch 124/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7267 - categorical_accuracy: 0.6611    \n",
      "Epoch 125/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7288 - categorical_accuracy: 0.6568    \n",
      "Epoch 126/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7272 - categorical_accuracy: 0.6600    \n",
      "Epoch 127/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7288 - categorical_accuracy: 0.6568    \n",
      "Epoch 128/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7289 - categorical_accuracy: 0.6583    \n",
      "Epoch 129/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7203 - categorical_accuracy: 0.6611    \n",
      "Epoch 130/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7237 - categorical_accuracy: 0.6584    \n",
      "Epoch 131/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7257 - categorical_accuracy: 0.6566    \n",
      "Epoch 132/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7241 - categorical_accuracy: 0.6588    \n",
      "Epoch 133/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7224 - categorical_accuracy: 0.6577    \n",
      "Epoch 134/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7194 - categorical_accuracy: 0.6617    \n",
      "Epoch 135/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7182 - categorical_accuracy: 0.6622    \n",
      "Epoch 136/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7204 - categorical_accuracy: 0.6616    \n",
      "Epoch 137/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7215 - categorical_accuracy: 0.6596    \n",
      "Epoch 138/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7178 - categorical_accuracy: 0.6633    \n",
      "Epoch 139/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7216 - categorical_accuracy: 0.6602    \n",
      "Epoch 140/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7192 - categorical_accuracy: 0.6636    \n",
      "Epoch 141/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7239 - categorical_accuracy: 0.6635    \n",
      "Epoch 142/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7124 - categorical_accuracy: 0.6674    \n",
      "Epoch 143/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25728/25728 [==============================] - 14s - loss: 0.7046 - categorical_accuracy: 0.6750    \n",
      "Epoch 144/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7099 - categorical_accuracy: 0.6715    \n",
      "Epoch 145/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7051 - categorical_accuracy: 0.6754    \n",
      "Epoch 146/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.7016 - categorical_accuracy: 0.6795    \n",
      "Epoch 147/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.6984 - categorical_accuracy: 0.6838    \n",
      "Epoch 148/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6880 - categorical_accuracy: 0.6878    \n",
      "Epoch 149/400\n",
      "25728/25728 [==============================] - 20s - loss: 0.6834 - categorical_accuracy: 0.6922    \n",
      "Epoch 150/400\n",
      "25728/25728 [==============================] - 17s - loss: 0.6885 - categorical_accuracy: 0.6903    \n",
      "Epoch 151/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6840 - categorical_accuracy: 0.6936    \n",
      "Epoch 152/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6816 - categorical_accuracy: 0.6940    \n",
      "Epoch 153/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6844 - categorical_accuracy: 0.6922    \n",
      "Epoch 154/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6760 - categorical_accuracy: 0.6983    \n",
      "Epoch 155/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6788 - categorical_accuracy: 0.6959    \n",
      "Epoch 156/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6772 - categorical_accuracy: 0.6964    \n",
      "Epoch 157/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6762 - categorical_accuracy: 0.6931    \n",
      "Epoch 158/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6749 - categorical_accuracy: 0.6972    \n",
      "Epoch 159/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6767 - categorical_accuracy: 0.6968    \n",
      "Epoch 160/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6714 - categorical_accuracy: 0.7013    \n",
      "Epoch 161/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6712 - categorical_accuracy: 0.6997    \n",
      "Epoch 162/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6738 - categorical_accuracy: 0.7003    \n",
      "Epoch 163/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6698 - categorical_accuracy: 0.7019    \n",
      "Epoch 164/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6712 - categorical_accuracy: 0.7015    \n",
      "Epoch 165/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6711 - categorical_accuracy: 0.7007    \n",
      "Epoch 166/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6679 - categorical_accuracy: 0.7026    \n",
      "Epoch 167/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6661 - categorical_accuracy: 0.7029    \n",
      "Epoch 168/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6656 - categorical_accuracy: 0.7030    \n",
      "Epoch 169/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6655 - categorical_accuracy: 0.7024    \n",
      "Epoch 170/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6656 - categorical_accuracy: 0.7036    \n",
      "Epoch 171/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6639 - categorical_accuracy: 0.7036    \n",
      "Epoch 172/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6598 - categorical_accuracy: 0.7079    \n",
      "Epoch 173/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6626 - categorical_accuracy: 0.7079    \n",
      "Epoch 174/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6586 - categorical_accuracy: 0.7108    \n",
      "Epoch 175/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6582 - categorical_accuracy: 0.7088    \n",
      "Epoch 176/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6551 - categorical_accuracy: 0.7088    \n",
      "Epoch 177/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.6564 - categorical_accuracy: 0.7123    \n",
      "Epoch 178/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.6545 - categorical_accuracy: 0.7134    \n",
      "Epoch 179/400\n",
      "25728/25728 [==============================] - 14s - loss: 0.6541 - categorical_accuracy: 0.7127    \n",
      "Epoch 180/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.6509 - categorical_accuracy: 0.7135    \n",
      "Epoch 181/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.6523 - categorical_accuracy: 0.7135    \n",
      "Epoch 182/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.6508 - categorical_accuracy: 0.7102    \n",
      "Epoch 183/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.6484 - categorical_accuracy: 0.7153    \n",
      "Epoch 184/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.6496 - categorical_accuracy: 0.7118    \n",
      "Epoch 185/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.6483 - categorical_accuracy: 0.7116    \n",
      "Epoch 186/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.6494 - categorical_accuracy: 0.7130    \n",
      "Epoch 187/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.6420 - categorical_accuracy: 0.7158    \n",
      "Epoch 188/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.6459 - categorical_accuracy: 0.7175    \n",
      "Epoch 189/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.6456 - categorical_accuracy: 0.7162    \n",
      "Epoch 190/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.6434 - categorical_accuracy: 0.7174    \n",
      "Epoch 191/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6391 - categorical_accuracy: 0.7172    \n",
      "Epoch 192/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.6409 - categorical_accuracy: 0.7150    \n",
      "Epoch 193/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.6379 - categorical_accuracy: 0.7195    \n",
      "Epoch 194/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6396 - categorical_accuracy: 0.7201    \n",
      "Epoch 195/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6391 - categorical_accuracy: 0.7184    \n",
      "Epoch 196/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.6349 - categorical_accuracy: 0.7225    \n",
      "Epoch 197/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.6312 - categorical_accuracy: 0.7240    \n",
      "Epoch 198/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6300 - categorical_accuracy: 0.7241    \n",
      "Epoch 199/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6323 - categorical_accuracy: 0.7222    \n",
      "Epoch 200/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6281 - categorical_accuracy: 0.7223    \n",
      "Epoch 201/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6284 - categorical_accuracy: 0.7278    \n",
      "Epoch 202/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6287 - categorical_accuracy: 0.7239    \n",
      "Epoch 203/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6280 - categorical_accuracy: 0.7265    \n",
      "Epoch 204/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6257 - categorical_accuracy: 0.7268    \n",
      "Epoch 205/400\n",
      "25728/25728 [==============================] - 18s - loss: 0.6278 - categorical_accuracy: 0.7236    \n",
      "Epoch 206/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6216 - categorical_accuracy: 0.7294    \n",
      "Epoch 207/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6235 - categorical_accuracy: 0.7260    \n",
      "Epoch 208/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6218 - categorical_accuracy: 0.7299    \n",
      "Epoch 209/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6199 - categorical_accuracy: 0.7309    \n",
      "Epoch 210/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6182 - categorical_accuracy: 0.7323    \n",
      "Epoch 211/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6163 - categorical_accuracy: 0.7325    \n",
      "Epoch 212/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6139 - categorical_accuracy: 0.7323    \n",
      "Epoch 213/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6139 - categorical_accuracy: 0.7329    \n",
      "Epoch 214/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25728/25728 [==============================] - 16s - loss: 0.6132 - categorical_accuracy: 0.7320    \n",
      "Epoch 215/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6132 - categorical_accuracy: 0.7321    \n",
      "Epoch 216/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6105 - categorical_accuracy: 0.7370    \n",
      "Epoch 217/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6110 - categorical_accuracy: 0.7347    \n",
      "Epoch 218/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6093 - categorical_accuracy: 0.7348    \n",
      "Epoch 219/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6005 - categorical_accuracy: 0.7370    \n",
      "Epoch 220/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6058 - categorical_accuracy: 0.7371    \n",
      "Epoch 221/400\n",
      "25728/25728 [==============================] - 17s - loss: 0.6040 - categorical_accuracy: 0.7377    \n",
      "Epoch 222/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6015 - categorical_accuracy: 0.7396    \n",
      "Epoch 223/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.6036 - categorical_accuracy: 0.7397    \n",
      "Epoch 224/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5999 - categorical_accuracy: 0.7422    \n",
      "Epoch 225/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5937 - categorical_accuracy: 0.7412    \n",
      "Epoch 226/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5964 - categorical_accuracy: 0.7413    \n",
      "Epoch 227/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5955 - categorical_accuracy: 0.7427    \n",
      "Epoch 228/400\n",
      "25728/25728 [==============================] - 17s - loss: 0.5933 - categorical_accuracy: 0.7430    \n",
      "Epoch 229/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5904 - categorical_accuracy: 0.7450    \n",
      "Epoch 230/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5925 - categorical_accuracy: 0.7462    \n",
      "Epoch 231/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5904 - categorical_accuracy: 0.7439    \n",
      "Epoch 232/400\n",
      "25728/25728 [==============================] - 17s - loss: 0.5889 - categorical_accuracy: 0.7430    \n",
      "Epoch 233/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5875 - categorical_accuracy: 0.7473    \n",
      "Epoch 234/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5860 - categorical_accuracy: 0.7485    \n",
      "Epoch 235/400\n",
      "25728/25728 [==============================] - 17s - loss: 0.5810 - categorical_accuracy: 0.7532    \n",
      "Epoch 236/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5850 - categorical_accuracy: 0.7456    \n",
      "Epoch 237/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5788 - categorical_accuracy: 0.7488    \n",
      "Epoch 238/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5788 - categorical_accuracy: 0.7509    \n",
      "Epoch 239/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5783 - categorical_accuracy: 0.7512    \n",
      "Epoch 240/400\n",
      "25728/25728 [==============================] - 17s - loss: 0.5758 - categorical_accuracy: 0.7526    \n",
      "Epoch 241/400\n",
      "25728/25728 [==============================] - 17s - loss: 0.5787 - categorical_accuracy: 0.7510    \n",
      "Epoch 242/400\n",
      "25728/25728 [==============================] - 17s - loss: 0.5737 - categorical_accuracy: 0.7540    \n",
      "Epoch 243/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5730 - categorical_accuracy: 0.7562    \n",
      "Epoch 244/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5701 - categorical_accuracy: 0.7554    \n",
      "Epoch 245/400\n",
      "25728/25728 [==============================] - 17s - loss: 0.5768 - categorical_accuracy: 0.7497    \n",
      "Epoch 246/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5674 - categorical_accuracy: 0.7569    \n",
      "Epoch 247/400\n",
      "25728/25728 [==============================] - 17s - loss: 0.5692 - categorical_accuracy: 0.7570    \n",
      "Epoch 248/400\n",
      "25728/25728 [==============================] - 17s - loss: 0.5690 - categorical_accuracy: 0.7586    \n",
      "Epoch 249/400\n",
      "25728/25728 [==============================] - 17s - loss: 0.5673 - categorical_accuracy: 0.7578    \n",
      "Epoch 250/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5610 - categorical_accuracy: 0.7604    \n",
      "Epoch 251/400\n",
      "25728/25728 [==============================] - 17s - loss: 0.5625 - categorical_accuracy: 0.7605    \n",
      "Epoch 252/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.5592 - categorical_accuracy: 0.7593    \n",
      "Epoch 253/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.5603 - categorical_accuracy: 0.7604    \n",
      "Epoch 254/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.5570 - categorical_accuracy: 0.7625    \n",
      "Epoch 255/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.5576 - categorical_accuracy: 0.7626    \n",
      "Epoch 256/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.5548 - categorical_accuracy: 0.7635    \n",
      "Epoch 257/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.5484 - categorical_accuracy: 0.7661    \n",
      "Epoch 258/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.5518 - categorical_accuracy: 0.7643    \n",
      "Epoch 259/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.5505 - categorical_accuracy: 0.7632    \n",
      "Epoch 260/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.5515 - categorical_accuracy: 0.7653    \n",
      "Epoch 261/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.5477 - categorical_accuracy: 0.7657    \n",
      "Epoch 262/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.5466 - categorical_accuracy: 0.7697    \n",
      "Epoch 263/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.5469 - categorical_accuracy: 0.7670    \n",
      "Epoch 264/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.5386 - categorical_accuracy: 0.7723    \n",
      "Epoch 265/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.5452 - categorical_accuracy: 0.7695    \n",
      "Epoch 266/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.5384 - categorical_accuracy: 0.7734    \n",
      "Epoch 267/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.5382 - categorical_accuracy: 0.7683    \n",
      "Epoch 268/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.5422 - categorical_accuracy: 0.7692    \n",
      "Epoch 269/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5398 - categorical_accuracy: 0.7703    \n",
      "Epoch 270/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5376 - categorical_accuracy: 0.7700    \n",
      "Epoch 271/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.5345 - categorical_accuracy: 0.7735    \n",
      "Epoch 272/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.5308 - categorical_accuracy: 0.7768    \n",
      "Epoch 273/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5317 - categorical_accuracy: 0.7734    \n",
      "Epoch 274/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5300 - categorical_accuracy: 0.7744    \n",
      "Epoch 275/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5315 - categorical_accuracy: 0.7765    \n",
      "Epoch 276/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5293 - categorical_accuracy: 0.7770    \n",
      "Epoch 277/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5245 - categorical_accuracy: 0.7802    \n",
      "Epoch 278/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5228 - categorical_accuracy: 0.7770    \n",
      "Epoch 279/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5222 - categorical_accuracy: 0.7786    \n",
      "Epoch 280/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5220 - categorical_accuracy: 0.7787    \n",
      "Epoch 281/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5186 - categorical_accuracy: 0.7787    \n",
      "Epoch 282/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5199 - categorical_accuracy: 0.7795    \n",
      "Epoch 283/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5121 - categorical_accuracy: 0.7842    \n",
      "Epoch 284/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5171 - categorical_accuracy: 0.7830    \n",
      "Epoch 285/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25728/25728 [==============================] - 16s - loss: 0.5177 - categorical_accuracy: 0.7797    \n",
      "Epoch 286/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5134 - categorical_accuracy: 0.7835    \n",
      "Epoch 287/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5121 - categorical_accuracy: 0.7854    \n",
      "Epoch 288/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5126 - categorical_accuracy: 0.7808    \n",
      "Epoch 289/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5095 - categorical_accuracy: 0.7855    \n",
      "Epoch 290/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5080 - categorical_accuracy: 0.7848    \n",
      "Epoch 291/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5071 - categorical_accuracy: 0.7875    \n",
      "Epoch 292/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5012 - categorical_accuracy: 0.7898    \n",
      "Epoch 293/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5033 - categorical_accuracy: 0.7866    \n",
      "Epoch 294/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4976 - categorical_accuracy: 0.7935    \n",
      "Epoch 295/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5018 - categorical_accuracy: 0.7906    \n",
      "Epoch 296/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5028 - categorical_accuracy: 0.7905    \n",
      "Epoch 297/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5004 - categorical_accuracy: 0.7880    \n",
      "Epoch 298/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.5002 - categorical_accuracy: 0.7917    \n",
      "Epoch 299/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4932 - categorical_accuracy: 0.7943    \n",
      "Epoch 300/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4946 - categorical_accuracy: 0.7922    \n",
      "Epoch 301/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4944 - categorical_accuracy: 0.7925    \n",
      "Epoch 302/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4886 - categorical_accuracy: 0.7951    \n",
      "Epoch 303/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4874 - categorical_accuracy: 0.7985    \n",
      "Epoch 304/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4888 - categorical_accuracy: 0.7962    \n",
      "Epoch 305/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4927 - categorical_accuracy: 0.7942    \n",
      "Epoch 306/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4885 - categorical_accuracy: 0.7933    \n",
      "Epoch 307/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4857 - categorical_accuracy: 0.7961    \n",
      "Epoch 308/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4853 - categorical_accuracy: 0.7973    \n",
      "Epoch 309/400\n",
      "25728/25728 [==============================] - 17s - loss: 0.4851 - categorical_accuracy: 0.7975    \n",
      "Epoch 310/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4776 - categorical_accuracy: 0.8012    \n",
      "Epoch 311/400\n",
      "25728/25728 [==============================] - 17s - loss: 0.4827 - categorical_accuracy: 0.7984    \n",
      "Epoch 312/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4828 - categorical_accuracy: 0.7987    \n",
      "Epoch 313/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4757 - categorical_accuracy: 0.8015    \n",
      "Epoch 314/400\n",
      "25728/25728 [==============================] - 17s - loss: 0.4729 - categorical_accuracy: 0.8033    \n",
      "Epoch 315/400\n",
      "25728/25728 [==============================] - 17s - loss: 0.4751 - categorical_accuracy: 0.8032    \n",
      "Epoch 316/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4745 - categorical_accuracy: 0.8050    \n",
      "Epoch 317/400\n",
      "25728/25728 [==============================] - 17s - loss: 0.4685 - categorical_accuracy: 0.8032    \n",
      "Epoch 318/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4783 - categorical_accuracy: 0.7987    \n",
      "Epoch 319/400\n",
      "25728/25728 [==============================] - 17s - loss: 0.4729 - categorical_accuracy: 0.8025    \n",
      "Epoch 320/400\n",
      "25728/25728 [==============================] - 17s - loss: 0.4683 - categorical_accuracy: 0.8043    \n",
      "Epoch 321/400\n",
      "25728/25728 [==============================] - 17s - loss: 0.4695 - categorical_accuracy: 0.8062    \n",
      "Epoch 322/400\n",
      "25728/25728 [==============================] - 17s - loss: 0.4628 - categorical_accuracy: 0.8089    \n",
      "Epoch 323/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4642 - categorical_accuracy: 0.8096    \n",
      "Epoch 324/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4619 - categorical_accuracy: 0.8075    \n",
      "Epoch 325/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4620 - categorical_accuracy: 0.8072    \n",
      "Epoch 326/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4585 - categorical_accuracy: 0.8117    \n",
      "Epoch 327/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4604 - categorical_accuracy: 0.8092    \n",
      "Epoch 328/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.4607 - categorical_accuracy: 0.8095    \n",
      "Epoch 329/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.4641 - categorical_accuracy: 0.8087    \n",
      "Epoch 330/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.4568 - categorical_accuracy: 0.8105    \n",
      "Epoch 331/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.4519 - categorical_accuracy: 0.8142    \n",
      "Epoch 332/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.4508 - categorical_accuracy: 0.8142    \n",
      "Epoch 333/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.4534 - categorical_accuracy: 0.8130    \n",
      "Epoch 334/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.4522 - categorical_accuracy: 0.8112    \n",
      "Epoch 335/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.4448 - categorical_accuracy: 0.8163    \n",
      "Epoch 336/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.4519 - categorical_accuracy: 0.8144    \n",
      "Epoch 337/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.4471 - categorical_accuracy: 0.8153    \n",
      "Epoch 338/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.4498 - categorical_accuracy: 0.8156    \n",
      "Epoch 339/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.4486 - categorical_accuracy: 0.8153    \n",
      "Epoch 340/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.4415 - categorical_accuracy: 0.8198    \n",
      "Epoch 341/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.4465 - categorical_accuracy: 0.8170    \n",
      "Epoch 342/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.4404 - categorical_accuracy: 0.8192    \n",
      "Epoch 343/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4399 - categorical_accuracy: 0.8199    \n",
      "Epoch 344/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.4413 - categorical_accuracy: 0.8171    \n",
      "Epoch 345/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.4400 - categorical_accuracy: 0.8209    \n",
      "Epoch 346/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.4365 - categorical_accuracy: 0.8218    \n",
      "Epoch 347/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.4370 - categorical_accuracy: 0.8205    \n",
      "Epoch 348/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4366 - categorical_accuracy: 0.8203    \n",
      "Epoch 349/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4339 - categorical_accuracy: 0.8218    \n",
      "Epoch 350/400\n",
      "25728/25728 [==============================] - 15s - loss: 0.4347 - categorical_accuracy: 0.8216    \n",
      "Epoch 351/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4320 - categorical_accuracy: 0.8242    \n",
      "Epoch 352/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4336 - categorical_accuracy: 0.8227    \n",
      "Epoch 353/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4332 - categorical_accuracy: 0.8207    \n",
      "Epoch 354/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4274 - categorical_accuracy: 0.8263    \n",
      "Epoch 355/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4288 - categorical_accuracy: 0.8246    \n",
      "Epoch 356/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25728/25728 [==============================] - 16s - loss: 0.4283 - categorical_accuracy: 0.8253    \n",
      "Epoch 357/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4291 - categorical_accuracy: 0.8232    \n",
      "Epoch 358/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4277 - categorical_accuracy: 0.8258    \n",
      "Epoch 359/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4174 - categorical_accuracy: 0.8312    \n",
      "Epoch 360/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4246 - categorical_accuracy: 0.8252    \n",
      "Epoch 361/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4212 - categorical_accuracy: 0.8279    \n",
      "Epoch 362/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4223 - categorical_accuracy: 0.8275    \n",
      "Epoch 363/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4213 - categorical_accuracy: 0.8288    \n",
      "Epoch 364/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4205 - categorical_accuracy: 0.8308    \n",
      "Epoch 365/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4152 - categorical_accuracy: 0.8287    \n",
      "Epoch 366/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4198 - categorical_accuracy: 0.8279    \n",
      "Epoch 367/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4195 - categorical_accuracy: 0.8276    \n",
      "Epoch 368/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4140 - categorical_accuracy: 0.8309    \n",
      "Epoch 369/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4131 - categorical_accuracy: 0.8328    \n",
      "Epoch 370/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4120 - categorical_accuracy: 0.8303    \n",
      "Epoch 371/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4146 - categorical_accuracy: 0.8352    \n",
      "Epoch 372/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4094 - categorical_accuracy: 0.8352    \n",
      "Epoch 373/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4137 - categorical_accuracy: 0.8304    \n",
      "Epoch 374/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4070 - categorical_accuracy: 0.8359    \n",
      "Epoch 375/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4064 - categorical_accuracy: 0.8349    \n",
      "Epoch 376/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4038 - categorical_accuracy: 0.8390    \n",
      "Epoch 377/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4022 - categorical_accuracy: 0.8366    \n",
      "Epoch 378/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4091 - categorical_accuracy: 0.8316    \n",
      "Epoch 379/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4039 - categorical_accuracy: 0.8358    \n",
      "Epoch 380/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4046 - categorical_accuracy: 0.8350    \n",
      "Epoch 381/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4013 - categorical_accuracy: 0.8365    \n",
      "Epoch 382/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4023 - categorical_accuracy: 0.8370    \n",
      "Epoch 383/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4005 - categorical_accuracy: 0.8363    \n",
      "Epoch 384/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.4003 - categorical_accuracy: 0.8372    \n",
      "Epoch 385/400\n",
      "25728/25728 [==============================] - 17s - loss: 0.3977 - categorical_accuracy: 0.8380    \n",
      "Epoch 386/400\n",
      "25728/25728 [==============================] - 18s - loss: 0.3999 - categorical_accuracy: 0.8387    \n",
      "Epoch 387/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.3972 - categorical_accuracy: 0.8389    \n",
      "Epoch 388/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.3898 - categorical_accuracy: 0.8422    \n",
      "Epoch 389/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.3988 - categorical_accuracy: 0.8391    \n",
      "Epoch 390/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.3913 - categorical_accuracy: 0.8410    \n",
      "Epoch 391/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.3906 - categorical_accuracy: 0.8410    \n",
      "Epoch 392/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.3908 - categorical_accuracy: 0.8415    \n",
      "Epoch 393/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.3895 - categorical_accuracy: 0.8417    \n",
      "Epoch 394/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.3914 - categorical_accuracy: 0.8427    \n",
      "Epoch 395/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.3967 - categorical_accuracy: 0.8400    \n",
      "Epoch 396/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.3820 - categorical_accuracy: 0.8489    \n",
      "Epoch 397/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.3914 - categorical_accuracy: 0.8425    \n",
      "Epoch 398/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.3853 - categorical_accuracy: 0.8438    \n",
      "Epoch 399/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.3836 - categorical_accuracy: 0.8440    \n",
      "Epoch 400/400\n",
      "25728/25728 [==============================] - 16s - loss: 0.3869 - categorical_accuracy: 0.8435    \n"
     ]
    }
   ],
   "source": [
    "#features = ['Volume_BTC', 'Bitcoin_Adj', 'Price_lagged']\n",
    "features = ['Volume_BTC', 'Price_lagged']\n",
    "\n",
    "rnn = build_model(features, data_length) \n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "history = rnn.fit(\n",
    "    [\n",
    "        #X_train_timestamp,\n",
    "        X_train_volume,\n",
    "        #X_train_trends,\n",
    "        X_train_lagged_price\n",
    "    ],\n",
    "    [\n",
    "        Y_train_is_spike_onehot\n",
    "    ]\n",
    "    ,\n",
    "#     validation_data=(\n",
    "#         [\n",
    "#             #X_test_timestamp,\n",
    "#             X_test_volume,\n",
    "#             #X_test_trends,\n",
    "#             X_test_lagged_price\n",
    "#         ],\n",
    "#         [\n",
    "#             Y_test_is_spike_onehot\n",
    "#         ]),\n",
    "    epochs=400,\n",
    "    batch_size=64,\n",
    "    callbacks=[\n",
    "      tensorboard_callback\n",
    "    ],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREKQiruISAj6f0SdDnLzOM43AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f2108f3198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "#plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4416/4524 [============================>.] - ETA: 0s\n",
      "\n",
      "Accuracy: 31.90%\n"
     ]
    }
   ],
   "source": [
    "score = rnn.evaluate(\n",
    "    [\n",
    "        #X_test_timestamp,\n",
    "        X_test_volume,\n",
    "        #X_test_trends,\n",
    "        X_test_lagged_price\n",
    "    ],\n",
    "    [\n",
    "        Y_test_is_spike_onehot\n",
    "    ])\n",
    "\n",
    "print('\\n')\n",
    "print(\"Accuracy: %.2f%%\" % (score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To test if Google Trends actually have any benefit in predicting spikes, I ran one with and without the trend data as input. \n",
    "\n",
    "#### For \"Is Spike\" cutoff of 0.1, (meaning Is Spike marks only the 10% biggest changes)\n",
    "    * With trend data, accuracy was 78.90% on test data.\n",
    "    * Without trend data, accuracy was 82.93% on test data.\n",
    "    \n",
    "#### For \"Is Spike\" cutoff of 0.3, \n",
    "    * With trend data, accuracy was 84.57% for epoch=40 and 89.69% for epoch=60, 87.98% for epoch=100, and 92.02% for epoch=200\n",
    "    * Without trend data, accuracy was 78.40% for epoch=40 and 88.88% for epoch=60, 93.60% for epoch=100, and 90.14% for epoch=200\n",
    "    \n",
    "    \n",
    "    * Accuracy on test data is much better than that of train data \n",
    "        * -> could be because the test data is statistically different than train data \n",
    "            * which makese sense because test data is the real big spike \n",
    "                    * Since I am using 10 hours of data to predict the next hour, it would make sense that the accuracy is good during this time since this is the time that people were looking up Bitcoin and perhaps buying them a few hours later \n",
    "                * get more up-to-date data \n",
    "        * OR BECAUSE TRAIN AND TEST DATA SOMEHOW OVERLAPS?\n",
    "        \n",
    "#### With updated data\n",
    "    * With trend data, accuracy was 45.12% for epoch=100, 45.56% for epoch=200 -> overfitting?\n",
    "        * -> put in dropout \n",
    "    - Without trend data, accuracy was 43.11% for epoch=100, 43.97% for epoch=200\n",
    "    \n",
    "    * with the updated data, the test data is now from December 20th, which is right after the massive spike already happened, to June\n",
    "    * Before, test data used to be from start of October to April-ish\n",
    "    \n",
    "#### With Updated Data and with Dropout of 0.2 \n",
    "    * With trend data, accuracy was 38.61% for epoch=150\n",
    "    * Without trend data, accuracy was 38.87% for epoch=150\n",
    "    \n",
    "    TODO: Increase data_length (memory in LSTM) and change dropout \n",
    "    \n",
    "#### Data_length=12 with Dropout of 0.2 \n",
    "    * With trend data, accuracy was 38.44% for epoch=400\n",
    "    * Without trend data, accuracy was 31.90% for epoch=400\n",
    "    \n",
    "#### Increasing data_length to 20 (with dropout)\n",
    "    * With trend data, accuracy was 38.23% with epoch=150\n",
    "    \n",
    "#### Data_length of 20 without dropout \n",
    "    * With trend data, accuracy was 46.90% with epoch=100\n",
    "    * Without trend data, accuracy was 46.13% with epoch=100\n",
    "    \n",
    "#### Data_length of 24 without dropout\n",
    "    * With trend data, accuracy was 44.17% with epoch=200\n",
    "    \n",
    "#### Data_length of 35 without dropout\n",
    "    * With trend data, accuracy was 46.8% with epoch=100\n",
    "    * Without trend data, accuracy was 43.52% with epoch=100\n",
    "    \n",
    "#### Data_length of 50 without dropout \n",
    "    * With trend data, accuracy was 45.40% with epoch=100\n",
    "    * Without trend data, accuracy was 45.67% with epoch=100\n",
    "    \n",
    "    \n",
    "    \n",
    "    * Todo: for the really high accuracy ones, graph which date ranges are most accurate (maybe when it was spiking it had higher accuracy) \n",
    "    *Todo: graph the test data and change colors depending on if value was predicted correctly or not!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.52280313e-01,   3.59167723e-04,   5.47360480e-01],\n",
       "       [  9.30840611e-01,   8.56013969e-03,   6.05992079e-02],\n",
       "       [  9.88024890e-01,   5.18933339e-05,   1.19231120e-02],\n",
       "       ..., \n",
       "       [  9.85919118e-01,   1.14388512e-02,   2.64205295e-03],\n",
       "       [  9.98261154e-01,   6.32497904e-05,   1.67550833e-03],\n",
       "       [  9.95321453e-01,   4.53380664e-04,   4.22520470e-03]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    predicted  actual\n",
       "0          -1    -1.0\n",
       "1           0     0.0\n",
       "2           0     0.0\n",
       "3           0     0.0\n",
       "4           0     0.0\n",
       "5           0     0.0\n",
       "6           0     0.0\n",
       "7           0     0.0\n",
       "8           0     0.0\n",
       "9           0     0.0\n",
       "10          0     0.0\n",
       "11          0     0.0\n",
       "12          0     0.0\n",
       "13          0     0.0\n",
       "14         -1    -1.0\n",
       "15          0     0.0\n",
       "16          0     0.0\n",
       "17          0     0.0\n",
       "18         -1    -1.0\n",
       "19          0     0.0\n",
       "20          0     0.0\n",
       "21         -1    -1.0\n",
       "22          0     0.0\n",
       "23          0     0.0\n",
       "24          0     0.0\n",
       "25          0     0.0\n",
       "26          0     0.0\n",
       "27          0     0.0\n",
       "28         -1     0.0\n",
       "29          0     0.0\n",
       "..        ...     ...\n",
       "70          0     0.0\n",
       "71          0     0.0\n",
       "72          0     0.0\n",
       "73          0     0.0\n",
       "74          0     0.0\n",
       "75          0     0.0\n",
       "76          0     0.0\n",
       "77          0     0.0\n",
       "78         -1     0.0\n",
       "79          0     0.0\n",
       "80         -1    -1.0\n",
       "81          0     0.0\n",
       "82          0     0.0\n",
       "83          0     0.0\n",
       "84          0     0.0\n",
       "85          0     0.0\n",
       "86          1     1.0\n",
       "87          0     0.0\n",
       "88          0     0.0\n",
       "89          0     0.0\n",
       "90          0     0.0\n",
       "91          0     0.0\n",
       "92          0     0.0\n",
       "93          0     0.0\n",
       "94          0     0.0\n",
       "95          1     1.0\n",
       "96          0     0.0\n",
       "97          0     0.0\n",
       "98          0    -1.0\n",
       "99          0     0.0\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.92023809523809519"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yhat = rnn.predict( \n",
    "    [\n",
    "        #X_test_timestamp,\n",
    "        X_test_volume,\n",
    "        X_test_trends,\n",
    "        X_test_lagged_price\n",
    "    ],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "display(yhat)\n",
    "\n",
    "inverted_yhat = np.argmax(yhat,axis=1) #returns INDICES of max \n",
    "onehot_to_val_dict = {0: 0, 1: 1, 2:-1 }\n",
    "\n",
    "inverted_yhat_arr = np.asarray(inverted_yhat)\n",
    "predicted = [onehot_to_val_dict[i] for i in inverted_yhat_arr]\n",
    "\n",
    "\n",
    "df_pred_output = pd.DataFrame(predicted, columns=['predicted'])\n",
    "df_pred_output['actual'] = Y_test_is_spike\n",
    "#df_pred_output['index_output'] = inverted_yhat\n",
    "display(df_pred_output.head(100))\n",
    "\n",
    "correct = (df_pred_output['actual'].values == df_pred_output['predicted'].values)\n",
    "accuracy = correct.sum() / correct.size\n",
    "display(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON# serial \n",
    "model_json = rnn.to_json()\n",
    "with open(\"model_classification_v1data_200epochs_10length.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "rnn.save_weights(\"model_classification_v1data_200epochs_10length.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>-1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1.0</th>\n",
       "      <td>808</td>\n",
       "      <td>298</td>\n",
       "      <td>628</td>\n",
       "      <td>1734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>333</td>\n",
       "      <td>476</td>\n",
       "      <td>382</td>\n",
       "      <td>1191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>500</td>\n",
       "      <td>345</td>\n",
       "      <td>760</td>\n",
       "      <td>1605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1641</td>\n",
       "      <td>1119</td>\n",
       "      <td>1770</td>\n",
       "      <td>4530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    -1     0     1   All\n",
       "Actual                           \n",
       "-1.0        808   298   628  1734\n",
       "0.0         333   476   382  1191\n",
       "1.0         500   345   760  1605\n",
       "All        1641  1119  1770  4530"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "#print(metrics.confusion_matrix(df_pred_output['actual'].values, df_pred_output['predicted'].values,labels=[0,1,-1]))\n",
    "\n",
    "confusion_matrix = pd.crosstab(df_pred_output['actual'].values, df_pred_output['predicted'].values, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "\n",
    "display(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to check what the rnn actually learned \n",
    "# visualize predicted vs actual to get insight into this \n",
    "\n",
    "# try with instead of just 10% biggest changes, maybe with 25% \n",
    "# is it just learning from the previous prices, or is google trends actually helping \n",
    "# -> run rnn without google trends \n",
    "\n",
    "\n",
    "# I have a master_df_v2 now so try that - this one has 0.3 as cutoff for is Spike \n",
    "# Have to eventually get validation data - also get overall newer more data "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
