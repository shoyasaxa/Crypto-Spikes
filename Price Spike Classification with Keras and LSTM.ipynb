{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras import Input\n",
    "from keras.engine import Model\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Concatenate, concatenate\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# features is a list of strings of feature names \n",
    "\n",
    "def build_model(features, data_length):\n",
    "    \n",
    "    inputs_list = [] \n",
    "    for feature_name in features:\n",
    "        inputs_list.append((Input(shape=(data_length,1), name=feature_name)))\n",
    "    \n",
    "    layers = [] \n",
    "    for i, input_name in enumerate(inputs_list): \n",
    "        layers.append(LSTM(64, return_sequences=False, dropout=0.2)(inputs_list[i]) )\n",
    "        \n",
    "    output = concatenate(layers) \n",
    "    output = Dense(3, activation='softmax', name='IsSpike')(output)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs = inputs_list,\n",
    "        outputs = [output]\n",
    "    )\n",
    "    \n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    \n",
    "    return model    \n",
    "\n",
    "data_length = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Shape\n",
    "\n",
    "* Price  ----------> LSTM --\\\n",
    "* Google Trends ---> LSTM ---> Dense Layer -> Softmax -> Output: Is Spike (1,0,-1) \n",
    "* Volume ----------> LSTM --/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* Input: Price, Google Trends, and Volume for time t0-t9 (10 hours of data) \n",
    "* Output: Is Spike (1,0,-1) for t10 \n",
    "    * Using 10 hours (t0-t9) of Price, Google Trends, and Volume to predict the price movement at t11 (t10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shoya\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Shoya\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Shoya\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Volume_BTC</th>\n",
       "      <th>Bitcoin_Adj</th>\n",
       "      <th>Close</th>\n",
       "      <th>Price_lagged</th>\n",
       "      <th>Is Spike</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.776791</td>\n",
       "      <td>0.471970</td>\n",
       "      <td>0.484557</td>\n",
       "      <td>0.484557</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.463316</td>\n",
       "      <td>0.439996</td>\n",
       "      <td>0.538331</td>\n",
       "      <td>0.538331</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.725079</td>\n",
       "      <td>0.529463</td>\n",
       "      <td>0.520715</td>\n",
       "      <td>0.520715</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.210661</td>\n",
       "      <td>0.416611</td>\n",
       "      <td>0.566098</td>\n",
       "      <td>0.566098</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.594148</td>\n",
       "      <td>0.445509</td>\n",
       "      <td>0.568881</td>\n",
       "      <td>0.568881</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Volume_BTC  Bitcoin_Adj     Close  Price_lagged  Is Spike\n",
       "1    0.776791     0.471970  0.484557      0.484557       1.0\n",
       "2    0.463316     0.439996  0.538331      0.538331       1.0\n",
       "3    0.725079     0.529463  0.520715      0.520715      -1.0\n",
       "4    0.210661     0.416611  0.566098      0.566098       0.0\n",
       "5    0.594148     0.445509  0.568881      0.568881      -1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Volume_BTC</th>\n",
       "      <th>Bitcoin_Adj</th>\n",
       "      <th>Close</th>\n",
       "      <th>Price_lagged</th>\n",
       "      <th>Is Spike</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30285</th>\n",
       "      <td>0.455905</td>\n",
       "      <td>0.449846</td>\n",
       "      <td>0.546519</td>\n",
       "      <td>0.546519</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30286</th>\n",
       "      <td>0.308749</td>\n",
       "      <td>0.457405</td>\n",
       "      <td>0.549938</td>\n",
       "      <td>0.549938</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30287</th>\n",
       "      <td>0.632915</td>\n",
       "      <td>0.453625</td>\n",
       "      <td>0.563428</td>\n",
       "      <td>0.563428</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30288</th>\n",
       "      <td>0.629822</td>\n",
       "      <td>0.434124</td>\n",
       "      <td>0.506657</td>\n",
       "      <td>0.506657</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30289</th>\n",
       "      <td>0.564642</td>\n",
       "      <td>0.445374</td>\n",
       "      <td>0.493535</td>\n",
       "      <td>0.493535</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Volume_BTC  Bitcoin_Adj     Close  Price_lagged  Is Spike\n",
       "30285    0.455905     0.449846  0.546519      0.546519       1.0\n",
       "30286    0.308749     0.457405  0.549938      0.549938      -1.0\n",
       "30287    0.632915     0.453625  0.563428      0.563428       0.0\n",
       "30288    0.629822     0.434124  0.506657      0.506657       0.0\n",
       "30289    0.564642     0.445374  0.493535      0.493535      -1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "master_df = pd.read_csv('C:/Users/Shoya/surf/data/master_df_v3.csv', encoding='latin1')\n",
    "df = master_df[['Timestamp', 'Close', 'Volume_(BTC)', 'Volume_(Currency)', 'Date(UTC)', 'Bitcoin (Adj.Overlap)', \n",
    "               'Close Price % Change', 'Close Price % Change (Abs)', 'Is Spike']]\n",
    "\n",
    "# lag inputs depending on data_length \n",
    "df['Price_lagged'] = df['Close']#.shift(data_length)\n",
    "df['Volume_BTC'] = df['Volume_(BTC)']#.shift(data_length)\n",
    "df['Bitcoin_Adj'] = df['Bitcoin (Adj.Overlap)']#.shift(data_length)\n",
    "\n",
    "df = df.dropna()\n",
    "cols = ['Volume_BTC','Bitcoin_Adj', 'Close', 'Price_lagged']\n",
    "\n",
    "# Stationalize Data by taking log differences\n",
    "data_array = np.diff(np.log(df[cols]), axis=0)\n",
    "\n",
    "# Min-Max Scale \n",
    "\n",
    "scalers = {}\n",
    "datas = [] \n",
    "\n",
    "df_scaled = pd.DataFrame(columns=cols)\n",
    "\n",
    "\n",
    "############################################################\n",
    "#  Fix below - I am scaling the whole data set together, when I should scale the train and test datasets separately\n",
    "############################################################\n",
    "\n",
    "for i in range(len(cols)): \n",
    "    scalers[cols[i]] = MinMaxScaler()\n",
    "    #print('data', data_array[:,i])\n",
    "    \n",
    "    col_data = data_array[:,i]\n",
    "    col_data = np.reshape(col_data, (len(col_data), 1))\n",
    "    \n",
    "    data = scalers[cols[i]].fit_transform( col_data )\n",
    "    #print('scaled', data)\n",
    "    data = np.reshape(data, (1, len(data)))\n",
    "    df_scaled[cols[i]] = data[0]\n",
    "    \n",
    "df_scaled['Is Spike'] = df['Is Spike']\n",
    "df_scaled.dropna(inplace=True)\n",
    "display(df_scaled.head())\n",
    "display(df_scaled.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -1.,  1., ...,  1.,  1., -1.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# split and reshape data to feed into RNN\n",
    "\n",
    "# X_timestamp = df_scaled['Timestamp'].values\n",
    "X_volume = df_scaled['Volume_BTC'].values\n",
    "X_trends = df_scaled['Bitcoin_Adj'].values\n",
    "X_lagged_price = df_scaled['Price_lagged'].values\n",
    "\n",
    "Y_is_spike = df_scaled['Is Spike'].values \n",
    "\n",
    "train_size = int(len(X_volume) * 0.85)\n",
    "train_size = int(train_size/data_length) * data_length\n",
    "\n",
    "test_size_index = int(len(X_volume)/data_length)*data_length\n",
    "\n",
    "X_train_volume = []\n",
    "X_test_volume = [] \n",
    "X_train_trends = []\n",
    "X_test_trends = []\n",
    "X_train_lagged_price = []\n",
    "X_test_lagged_price = []\n",
    "Y_train_is_spike = [] \n",
    "Y_test_is_spike = [] \n",
    "\n",
    "for i in range(train_size-data_length):\n",
    "    vol_temp = []\n",
    "    trends_temp = []\n",
    "    price_temp = []\n",
    "    for j in range(data_length):\n",
    "        vol_temp.append(X_volume[i+j])\n",
    "        trends_temp.append(X_trends[i+j])\n",
    "        price_temp.append(X_lagged_price[i+j])\n",
    "    X_train_volume.append(vol_temp)\n",
    "    X_train_trends.append(trends_temp)\n",
    "    X_train_lagged_price.append(price_temp)\n",
    "    \n",
    "    Y_train_is_spike.append(Y_is_spike[i+data_length])\n",
    "\n",
    "for i in range(test_size_index-train_size-data_length):\n",
    "    vol_temp = []\n",
    "    trends_temp = [] \n",
    "    price_temp = [] \n",
    "    for j in range(data_length):\n",
    "        vol_temp.append(X_volume[train_size+i+j])\n",
    "        trends_temp.append(X_trends[train_size+i+j])\n",
    "        price_temp.append(X_lagged_price[train_size+i+j])\n",
    "    X_test_volume.append(vol_temp)\n",
    "    X_test_trends.append(trends_temp)\n",
    "    X_test_lagged_price.append(price_temp)\n",
    "    \n",
    "    Y_test_is_spike.append(Y_is_spike[train_size+i+data_length])\n",
    "    \n",
    "X_train_volume = np.array(X_train_volume)\n",
    "X_test_volume =  np.array(X_test_volume)\n",
    "X_train_trends = np.array(X_train_trends)\n",
    "X_test_trends = np.array(X_test_trends)\n",
    "X_train_lagged_price = np.array(X_train_lagged_price)\n",
    "X_test_lagged_price = np.array(X_test_lagged_price)\n",
    "Y_train_is_spike =  np.array(Y_train_is_spike)\n",
    "Y_test_is_spike = np.array(Y_test_is_spike)\n",
    "    \n",
    "    \n",
    "Y_train_is_spike_onehot = to_categorical(Y_train_is_spike, num_classes=3)\n",
    "Y_test_is_spike_onehot = to_categorical(Y_test_is_spike,num_classes=3)\n",
    "display(Y_train_is_spike)\n",
    "\n",
    "# y = pd.DataFrame(Y_train_is_spike_onehot)\n",
    "# y['actual'] = Y_train_is_spike\n",
    "# display(y.head(25))\n",
    "    \n",
    "# display(X_train_trends.shape)\n",
    "# display(Y_train_is_spike.shape)\n",
    "\n",
    "#display(X_train_lagged_price)\n",
    "#display(Y_train_is_spike)\n",
    "\n",
    "# df_train = pd.DataFrame(X_train_lagged_price)\n",
    "# df_train['label'] = Y_train_is_spike\n",
    "# display(df_train.tail(20))\n",
    "# display(df_scaled.head(30))\n",
    "# display(df_train.head(30))\n",
    "\n",
    "#--------------------------------\n",
    "\n",
    "# # X_train_timestamp, X_test_timestamp = X_timestamp[:train_size], X_timestamp[train_size:test_size_index ]\n",
    "# X_train_volume, X_test_volume = X_volume[:train_size], X_volume[train_size:test_size_index ]\n",
    "# X_train_trends, X_test_trends = X_trends[:train_size], X_trends[train_size:test_size_index ]\n",
    "# X_train_lagged_price, X_test_lagged_price = X_lagged_price[:train_size], X_lagged_price[train_size:test_size_index ]\n",
    "\n",
    "# # becasue I lagged the x inputs, I should forward the Y's by the data_length as well \n",
    "# Y_train_is_spike, Y_test_is_spike = Y_is_spike[data_length:train_size], Y_is_spike[train_size+data_length:test_size_index ]\n",
    "\n",
    "\n",
    "# # X.shape is (samples, timesteps, dimension) \n",
    "# # timestemps is 15, samples is just however many nobs there are (but it doesn't matter, so it should be None)\n",
    "\n",
    "\n",
    "X_train_volume = np.reshape(X_train_volume, (X_train_volume.shape[0],data_length,1) ) \n",
    "X_train_trends = np.reshape(X_train_trends, (X_train_trends.shape[0],data_length,1) ) \n",
    "X_train_lagged_price = np.reshape(X_train_lagged_price, (X_train_lagged_price.shape[0], data_length, 1))\n",
    "\n",
    "X_test_volume = np.reshape(X_test_volume, (X_test_volume.shape[0],data_length,1) ) \n",
    "X_test_trends = np.reshape(X_test_trends, (X_test_trends.shape[0],data_length,1) )  \n",
    "X_test_lagged_price = np.reshape(X_test_lagged_price, (X_test_lagged_price.shape[0],data_length,1))\n",
    "\n",
    "\n",
    "# # X_train_timestamp = np.reshape(X_train_timestamp, (int(X_train_timestamp.shape[0]/data_length),data_length,1) ) \n",
    "# X_train_volume = np.reshape(X_train_volume, (int(X_train_volume.shape[0]/data_length),data_length,1) ) \n",
    "# X_train_trends = np.reshape(X_train_trends, (int(X_train_trends.shape[0]/data_length),data_length,1) ) \n",
    "# X_train_lagged_price = np.reshape(X_train_lagged_price, (int(X_train_lagged_price.shape[0]/data_length), data_length, 1))\n",
    "\n",
    "# # X_test_timestamp = np.reshape(X_test_timestamp, (int(X_test_timestamp.shape[0]/data_length),data_length,1) ) \n",
    "# X_test_volume = np.reshape(X_test_volume, (int(X_test_volume.shape[0]/data_length),data_length,1) ) \n",
    "# X_test_trends = np.reshape(X_test_trends, (int(X_test_trends.shape[0]/data_length),data_length,1) )  \n",
    "# X_test_lagged_price = np.reshape(X_test_lagged_price, (int(X_test_lagged_price.shape[0]/data_length),data_length,1))\n",
    "\n",
    "\n",
    "# # Don't need the 1 for the third dimension for Y's??\n",
    "\n",
    "\n",
    "# Y_train_is_spike = np.reshape(Y_train_is_spike, (int(Y_train_is_spike.shape[0]/data_length),  data_length) ) \n",
    "# Y_test_is_spike = np.reshape(Y_test_is_spike, (int(Y_test_is_spike.shape[0]/data_length),  data_length) )\n",
    "\n",
    "#-----------------------------------\n",
    "\n",
    "\n",
    "# instead of using input 1,2,3,4,5,6,7,8,9,10 to predict output for 11,12,13,14,15,16,17,18,19,20\n",
    "# I want to use input 1,2,3,4,5,6,7,8,9,10 to predict output for 11, then 2,3,4,5,6,7,8,9,10,11 to predict output for 12 \n",
    "\n",
    "# right now I am actually feeding input 1,2,3,4,5,6,7,8,9,10 to predict output for 1,2,3,4,5,6,7,8,9,10. \n",
    "# instead I should at least feed 1,2,3..8,9,10 to predict 11,12,13,14,15,16,17,18,19,20 -> lag everything by data_length! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "25720/25720 [==============================] - 52s - loss: 1.0824 - categorical_accuracy: 0.4236    \n",
      "Epoch 2/150\n",
      "25720/25720 [==============================] - 51s - loss: 1.0818 - categorical_accuracy: 0.4242    \n",
      "Epoch 3/150\n",
      "25720/25720 [==============================] - 51s - loss: 1.0810 - categorical_accuracy: 0.4242    \n",
      "Epoch 4/150\n",
      "25720/25720 [==============================] - 51s - loss: 1.0767 - categorical_accuracy: 0.4273    \n",
      "Epoch 5/150\n",
      "25720/25720 [==============================] - 51s - loss: 1.0717 - categorical_accuracy: 0.4364    \n",
      "Epoch 6/150\n",
      "25720/25720 [==============================] - 51s - loss: 1.0657 - categorical_accuracy: 0.4526    \n",
      "Epoch 7/150\n",
      "25720/25720 [==============================] - 51s - loss: 1.0596 - categorical_accuracy: 0.4607    \n",
      "Epoch 8/150\n",
      "25720/25720 [==============================] - 51s - loss: 1.0555 - categorical_accuracy: 0.4699    \n",
      "Epoch 9/150\n",
      "25720/25720 [==============================] - 51s - loss: 1.0470 - categorical_accuracy: 0.4836    \n",
      "Epoch 10/150\n",
      "25720/25720 [==============================] - 51s - loss: 1.0281 - categorical_accuracy: 0.4964    \n",
      "Epoch 11/150\n",
      "25720/25720 [==============================] - 51s - loss: 1.0158 - categorical_accuracy: 0.5015    \n",
      "Epoch 12/150\n",
      "25720/25720 [==============================] - 51s - loss: 1.0072 - categorical_accuracy: 0.5094    \n",
      "Epoch 13/150\n",
      "25720/25720 [==============================] - 51s - loss: 0.9989 - categorical_accuracy: 0.5169    \n",
      "Epoch 14/150\n",
      "25720/25720 [==============================] - 51s - loss: 0.9943 - categorical_accuracy: 0.5217    \n",
      "Epoch 15/150\n",
      "25720/25720 [==============================] - 51s - loss: 0.9832 - categorical_accuracy: 0.5269    \n",
      "Epoch 16/150\n",
      "25720/25720 [==============================] - 51s - loss: 0.9752 - categorical_accuracy: 0.5384    \n",
      "Epoch 17/150\n",
      "25720/25720 [==============================] - 51s - loss: 0.9662 - categorical_accuracy: 0.5430    \n",
      "Epoch 18/150\n",
      "25720/25720 [==============================] - 50s - loss: 0.9545 - categorical_accuracy: 0.5510    \n",
      "Epoch 19/150\n",
      "25720/25720 [==============================] - 50s - loss: 0.9344 - categorical_accuracy: 0.5709    \n",
      "Epoch 20/150\n",
      "25720/25720 [==============================] - 50s - loss: 0.9176 - categorical_accuracy: 0.5780    \n",
      "Epoch 21/150\n",
      "25720/25720 [==============================] - 50s - loss: 0.9082 - categorical_accuracy: 0.5852    \n",
      "Epoch 22/150\n",
      "25720/25720 [==============================] - 50s - loss: 0.9011 - categorical_accuracy: 0.5935    \n",
      "Epoch 23/150\n",
      "25720/25720 [==============================] - 53s - loss: 0.8989 - categorical_accuracy: 0.5906    \n",
      "Epoch 24/150\n",
      "25720/25720 [==============================] - 47s - loss: 0.8853 - categorical_accuracy: 0.5991    \n",
      "Epoch 25/150\n",
      "25720/25720 [==============================] - 40s - loss: 0.8835 - categorical_accuracy: 0.5988    \n",
      "Epoch 26/150\n",
      "25720/25720 [==============================] - 42s - loss: 0.8836 - categorical_accuracy: 0.6014    \n",
      "Epoch 27/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.8750 - categorical_accuracy: 0.5988    \n",
      "Epoch 28/150\n",
      "25720/25720 [==============================] - 40s - loss: 0.8685 - categorical_accuracy: 0.6072    \n",
      "Epoch 29/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.8685 - categorical_accuracy: 0.6064    \n",
      "Epoch 30/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.8595 - categorical_accuracy: 0.6121    \n",
      "Epoch 31/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.8580 - categorical_accuracy: 0.6100     ETA: 2s\n",
      "Epoch 32/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.8463 - categorical_accuracy: 0.6161    \n",
      "Epoch 33/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.8480 - categorical_accuracy: 0.6145    \n",
      "Epoch 34/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.8414 - categorical_accuracy: 0.6182    \n",
      "Epoch 35/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.8439 - categorical_accuracy: 0.6173    \n",
      "Epoch 36/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.8379 - categorical_accuracy: 0.6194    \n",
      "Epoch 37/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.8367 - categorical_accuracy: 0.6208    \n",
      "Epoch 38/150\n",
      "25720/25720 [==============================] - 42s - loss: 0.8274 - categorical_accuracy: 0.6277    \n",
      "Epoch 39/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.8286 - categorical_accuracy: 0.6208    \n",
      "Epoch 40/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.8257 - categorical_accuracy: 0.6225    \n",
      "Epoch 41/150\n",
      "25720/25720 [==============================] - 42s - loss: 0.8247 - categorical_accuracy: 0.6215    \n",
      "Epoch 42/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.8226 - categorical_accuracy: 0.6250    \n",
      "Epoch 43/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.8170 - categorical_accuracy: 0.6260    \n",
      "Epoch 44/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.8159 - categorical_accuracy: 0.6292    \n",
      "Epoch 45/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.8135 - categorical_accuracy: 0.6294    \n",
      "Epoch 46/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.8124 - categorical_accuracy: 0.6278    \n",
      "Epoch 47/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.8109 - categorical_accuracy: 0.6293    \n",
      "Epoch 48/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.8100 - categorical_accuracy: 0.6272    \n",
      "Epoch 49/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.8000 - categorical_accuracy: 0.6307    \n",
      "Epoch 50/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.8024 - categorical_accuracy: 0.6321    \n",
      "Epoch 51/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7995 - categorical_accuracy: 0.6297    \n",
      "Epoch 52/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7954 - categorical_accuracy: 0.6343    \n",
      "Epoch 53/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7937 - categorical_accuracy: 0.6356    \n",
      "Epoch 54/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7972 - categorical_accuracy: 0.6320    \n",
      "Epoch 55/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7900 - categorical_accuracy: 0.6328    \n",
      "Epoch 56/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7916 - categorical_accuracy: 0.6336     ETA: 1s - loss: 0\n",
      "Epoch 57/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7914 - categorical_accuracy: 0.6337    \n",
      "Epoch 58/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7872 - categorical_accuracy: 0.6376    \n",
      "Epoch 59/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7878 - categorical_accuracy: 0.6346    \n",
      "Epoch 60/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7873 - categorical_accuracy: 0.6345    \n",
      "Epoch 61/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7855 - categorical_accuracy: 0.6353    \n",
      "Epoch 62/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7820 - categorical_accuracy: 0.6374    \n",
      "Epoch 63/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7810 - categorical_accuracy: 0.6383    \n",
      "Epoch 64/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7781 - categorical_accuracy: 0.6382    \n",
      "Epoch 65/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7791 - categorical_accuracy: 0.6407    \n",
      "Epoch 66/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7777 - categorical_accuracy: 0.6403    \n",
      "Epoch 67/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7778 - categorical_accuracy: 0.6386    \n",
      "Epoch 68/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7751 - categorical_accuracy: 0.6404    \n",
      "Epoch 69/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7750 - categorical_accuracy: 0.6398    \n",
      "Epoch 70/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7732 - categorical_accuracy: 0.6397    \n",
      "Epoch 71/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7723 - categorical_accuracy: 0.6426     ETA: 0s - loss: 0.7726 - categorical_accura\n",
      "Epoch 72/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25720/25720 [==============================] - 38s - loss: 0.7694 - categorical_accuracy: 0.6437    \n",
      "Epoch 73/150\n",
      "25720/25720 [==============================] - 38s - loss: 0.7738 - categorical_accuracy: 0.6373    \n",
      "Epoch 74/150\n",
      "25720/25720 [==============================] - 40s - loss: 0.7704 - categorical_accuracy: 0.6409    \n",
      "Epoch 75/150\n",
      "25720/25720 [==============================] - 38s - loss: 0.7704 - categorical_accuracy: 0.6420    \n",
      "Epoch 76/150\n",
      "25720/25720 [==============================] - 38s - loss: 0.7633 - categorical_accuracy: 0.6437    \n",
      "Epoch 77/150\n",
      "25720/25720 [==============================] - 39s - loss: 0.7625 - categorical_accuracy: 0.6478    \n",
      "Epoch 78/150\n",
      "25720/25720 [==============================] - 38s - loss: 0.7658 - categorical_accuracy: 0.6426    \n",
      "Epoch 79/150\n",
      "25720/25720 [==============================] - 38s - loss: 0.7671 - categorical_accuracy: 0.6425    \n",
      "Epoch 80/150\n",
      "25720/25720 [==============================] - 39s - loss: 0.7679 - categorical_accuracy: 0.6442    \n",
      "Epoch 81/150\n",
      "25720/25720 [==============================] - 39s - loss: 0.7657 - categorical_accuracy: 0.6419    \n",
      "Epoch 82/150\n",
      "25720/25720 [==============================] - 40s - loss: 0.7616 - categorical_accuracy: 0.6470    \n",
      "Epoch 83/150\n",
      "25720/25720 [==============================] - 39s - loss: 0.7629 - categorical_accuracy: 0.6425    \n",
      "Epoch 84/150\n",
      "25720/25720 [==============================] - 39s - loss: 0.7577 - categorical_accuracy: 0.6467    \n",
      "Epoch 85/150\n",
      "25720/25720 [==============================] - 39s - loss: 0.7593 - categorical_accuracy: 0.6450    \n",
      "Epoch 86/150\n",
      "25720/25720 [==============================] - 39s - loss: 0.7624 - categorical_accuracy: 0.6436    \n",
      "Epoch 87/150\n",
      "25720/25720 [==============================] - 39s - loss: 0.7637 - categorical_accuracy: 0.6423    \n",
      "Epoch 88/150\n",
      "25720/25720 [==============================] - 39s - loss: 0.7616 - categorical_accuracy: 0.6429    \n",
      "Epoch 89/150\n",
      "25720/25720 [==============================] - 39s - loss: 0.7564 - categorical_accuracy: 0.6456    \n",
      "Epoch 90/150\n",
      "25720/25720 [==============================] - 39s - loss: 0.7602 - categorical_accuracy: 0.6437    \n",
      "Epoch 91/150\n",
      "25720/25720 [==============================] - 40s - loss: 0.7631 - categorical_accuracy: 0.6462    \n",
      "Epoch 92/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7585 - categorical_accuracy: 0.6424    \n",
      "Epoch 93/150\n",
      "25720/25720 [==============================] - 39s - loss: 0.7583 - categorical_accuracy: 0.6483    \n",
      "Epoch 94/150\n",
      "25720/25720 [==============================] - 40s - loss: 0.7594 - categorical_accuracy: 0.6415    \n",
      "Epoch 95/150\n",
      "25720/25720 [==============================] - 40s - loss: 0.7533 - categorical_accuracy: 0.6497     ETA:  - ETA: 2s - loss: 0.7540 - categorical_accuracy: 0.64 - ETA: 2s - loss: 0 - ETA: 1s - loss: 0.7534 - catego\n",
      "Epoch 96/150\n",
      "25720/25720 [==============================] - 40s - loss: 0.7521 - categorical_accuracy: 0.6482    \n",
      "Epoch 97/150\n",
      "25720/25720 [==============================] - 40s - loss: 0.7524 - categorical_accuracy: 0.6473    \n",
      "Epoch 98/150\n",
      "25720/25720 [==============================] - 40s - loss: 0.7542 - categorical_accuracy: 0.6469    \n",
      "Epoch 99/150\n",
      "25720/25720 [==============================] - 40s - loss: 0.7520 - categorical_accuracy: 0.6455     ETA: 0s - loss: 0.7520 - categorical_accuracy: 0.64\n",
      "Epoch 100/150\n",
      "25720/25720 [==============================] - 40s - loss: 0.7526 - categorical_accuracy: 0.6457    \n",
      "Epoch 101/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7520 - categorical_accuracy: 0.6495    \n",
      "Epoch 102/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7490 - categorical_accuracy: 0.6509     ETA: 2s - l\n",
      "Epoch 103/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7521 - categorical_accuracy: 0.6453    \n",
      "Epoch 104/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7521 - categorical_accuracy: 0.6505    \n",
      "Epoch 105/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7480 - categorical_accuracy: 0.6476    \n",
      "Epoch 106/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7478 - categorical_accuracy: 0.6472    \n",
      "Epoch 107/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7521 - categorical_accuracy: 0.6490    \n",
      "Epoch 108/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7456 - categorical_accuracy: 0.6515    \n",
      "Epoch 109/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7460 - categorical_accuracy: 0.6508    \n",
      "Epoch 110/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7418 - categorical_accuracy: 0.6530    \n",
      "Epoch 111/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7412 - categorical_accuracy: 0.6516    \n",
      "Epoch 112/150\n",
      "25720/25720 [==============================] - 42s - loss: 0.7504 - categorical_accuracy: 0.6455    \n",
      "Epoch 113/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7429 - categorical_accuracy: 0.6524    \n",
      "Epoch 114/150\n",
      "25720/25720 [==============================] - 44s - loss: 0.7447 - categorical_accuracy: 0.6495    \n",
      "Epoch 115/150\n",
      "25720/25720 [==============================] - 46s - loss: 0.7416 - categorical_accuracy: 0.6506    \n",
      "Epoch 116/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7407 - categorical_accuracy: 0.6538    \n",
      "Epoch 117/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7443 - categorical_accuracy: 0.6538    \n",
      "Epoch 118/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7441 - categorical_accuracy: 0.6499    \n",
      "Epoch 119/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7447 - categorical_accuracy: 0.6514    \n",
      "Epoch 120/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7395 - categorical_accuracy: 0.6547    \n",
      "Epoch 121/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7469 - categorical_accuracy: 0.6490    \n",
      "Epoch 122/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7409 - categorical_accuracy: 0.6521    \n",
      "Epoch 123/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7409 - categorical_accuracy: 0.6531    \n",
      "Epoch 124/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7417 - categorical_accuracy: 0.6488    \n",
      "Epoch 125/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7407 - categorical_accuracy: 0.6548    \n",
      "Epoch 126/150\n",
      "25720/25720 [==============================] - 42s - loss: 0.7381 - categorical_accuracy: 0.6523    \n",
      "Epoch 127/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7388 - categorical_accuracy: 0.6515    \n",
      "Epoch 128/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7417 - categorical_accuracy: 0.6525    \n",
      "Epoch 129/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7385 - categorical_accuracy: 0.6523    \n",
      "Epoch 130/150\n",
      "25720/25720 [==============================] - 42s - loss: 0.7383 - categorical_accuracy: 0.6536    \n",
      "Epoch 131/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7401 - categorical_accuracy: 0.6516    \n",
      "Epoch 132/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7328 - categorical_accuracy: 0.6525    \n",
      "Epoch 133/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7406 - categorical_accuracy: 0.6538    \n",
      "Epoch 134/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7353 - categorical_accuracy: 0.6558    \n",
      "Epoch 135/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7383 - categorical_accuracy: 0.6512    \n",
      "Epoch 136/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7344 - categorical_accuracy: 0.6552    \n",
      "Epoch 137/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7355 - categorical_accuracy: 0.6521    \n",
      "Epoch 138/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7313 - categorical_accuracy: 0.6562    \n",
      "Epoch 139/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7375 - categorical_accuracy: 0.6517    \n",
      "Epoch 140/150\n",
      "25720/25720 [==============================] - 41s - loss: 0.7392 - categorical_accuracy: 0.6516    \n",
      "Epoch 141/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25720/25720 [==============================] - 39s - loss: 0.7345 - categorical_accuracy: 0.6568    \n",
      "Epoch 142/150\n",
      "25720/25720 [==============================] - 38s - loss: 0.7336 - categorical_accuracy: 0.6538    \n",
      "Epoch 143/150\n",
      "25720/25720 [==============================] - 38s - loss: 0.7347 - categorical_accuracy: 0.6552    \n",
      "Epoch 144/150\n",
      "25720/25720 [==============================] - 38s - loss: 0.7336 - categorical_accuracy: 0.6555    \n",
      "Epoch 145/150\n",
      "25720/25720 [==============================] - 38s - loss: 0.7310 - categorical_accuracy: 0.6503    \n",
      "Epoch 146/150\n",
      "25720/25720 [==============================] - 38s - loss: 0.7293 - categorical_accuracy: 0.6548    \n",
      "Epoch 147/150\n",
      "25720/25720 [==============================] - 38s - loss: 0.7294 - categorical_accuracy: 0.6574    \n",
      "Epoch 148/150\n",
      "25720/25720 [==============================] - 38s - loss: 0.7323 - categorical_accuracy: 0.6567    \n",
      "Epoch 149/150\n",
      "25720/25720 [==============================] - 38s - loss: 0.7330 - categorical_accuracy: 0.6537    \n",
      "Epoch 150/150\n",
      "25720/25720 [==============================] - 38s - loss: 0.7271 - categorical_accuracy: 0.6567    \n"
     ]
    }
   ],
   "source": [
    "features = ['Volume_BTC', 'Bitcoin_Adj', 'Price_lagged']\n",
    "#features = ['Volume_BTC', 'Price_lagged']\n",
    "\n",
    "rnn = build_model(features, data_length) \n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "history = rnn.fit(\n",
    "    [\n",
    "        #X_train_timestamp,\n",
    "        X_train_volume,\n",
    "        X_train_trends,\n",
    "        X_train_lagged_price\n",
    "    ],\n",
    "    [\n",
    "        Y_train_is_spike_onehot\n",
    "    ]\n",
    "    ,\n",
    "#     validation_data=(\n",
    "#         [\n",
    "#             #X_test_timestamp,\n",
    "#             X_test_volume,\n",
    "#             #X_test_trends,\n",
    "#             X_test_lagged_price\n",
    "#         ],\n",
    "#         [\n",
    "#             Y_test_is_spike_onehot\n",
    "#         ]),\n",
    "    epochs=150,\n",
    "    batch_size=64,\n",
    "    callbacks=[\n",
    "      tensorboard_callback\n",
    "    ],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cd5fb164e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "#plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4480/4530 [============================>.] - ETA: 0s\n",
      "\n",
      "Accuracy: 38.87%\n"
     ]
    }
   ],
   "source": [
    "score = rnn.evaluate(\n",
    "    [\n",
    "        #X_test_timestamp,\n",
    "        X_test_volume,\n",
    "        X_test_trends,\n",
    "        X_test_lagged_price\n",
    "    ],\n",
    "    [\n",
    "        Y_test_is_spike_onehot\n",
    "    ])\n",
    "\n",
    "print('\\n')\n",
    "print(\"Accuracy: %.2f%%\" % (score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To test if Google Trends actually have any benefit in predicting spikes, I ran one with and without the trend data as input. \n",
    "\n",
    "#### For \"Is Spike\" cutoff of 0.1, (meaning Is Spike marks only the 10% biggest changes)\n",
    "    * With trend data, accuracy was 78.90% on test data.\n",
    "    * Without trend data, accuracy was 82.93% on test data.\n",
    "    \n",
    "#### For \"Is Spike\" cutoff of 0.3, \n",
    "    * With trend data, accuracy was 84.57% for epoch=40 and 89.69% for epoch=60, and 87.98% for epoch=100\n",
    "    * Without trend data, accuracy was 78.40% for epoch=40 and 88.88% for epoch=60, and 93.60% for epoch=100\n",
    "    \n",
    "    Thus Google Trends actually helped Is Spike in the latter case!!!! \n",
    "    \n",
    "    \n",
    "    * Accuracy on test data is much better than that of train data \n",
    "        * -> could be because the test data is statistically different than train data \n",
    "            * which makese sense because test data is the real big spike \n",
    "                    * Since I am using 10 hours of data to predict the next hour, it would make sense that the accuracy is good during this time since this is the time that people were looking up Bitcoin and perhaps buying them a few hours later \n",
    "                * get more up-to-date data \n",
    "        * OR BECAUSE TRAIN AND TEST DATA SOMEHOW OVERLAPS?\n",
    "        \n",
    "#### With updated data\n",
    "    * With trend data, accuracy was 45.12% for epoch=100, 45.56% for epoch=200 -> overfitting?\n",
    "        * -> put in dropout \n",
    "    - Without trend data, accuracy was 43.11% for epoch=100, 43/97% for epoch=200\n",
    "    \n",
    "    * with the updated data, the test data is now from December 20th, which is right after the massive spike already happened\n",
    "    \n",
    "#### With Updated Data and with Dropout of 0.2 \n",
    "    * With trend data, accuracy was 38.61% for epoch=150\n",
    "    * Without trend data, accuracy was 38.87% for epoch=150\n",
    "    \n",
    "    TODO: Increase data_length (memory in LSTM) and change dropout \n",
    "    \n",
    "#### Increasing data_length to 20 \n",
    "    * With trend data, accuracy was X% with epoch=150\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02534344,  0.05034934,  0.92430729],\n",
       "       [ 0.03313668,  0.90681744,  0.06004593],\n",
       "       [ 0.43258792,  0.43481657,  0.1325956 ],\n",
       "       ..., \n",
       "       [ 0.37551153,  0.06019156,  0.56429696],\n",
       "       [ 0.53921092,  0.44531742,  0.01547169],\n",
       "       [ 0.17624743,  0.30557993,  0.51817256]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    predicted  actual\n",
       "0          -1    -1.0\n",
       "1           1     1.0\n",
       "2           1     1.0\n",
       "3          -1    -1.0\n",
       "4           1     1.0\n",
       "5           1     1.0\n",
       "6          -1    -1.0\n",
       "7          -1    -1.0\n",
       "8          -1    -1.0\n",
       "9          -1    -1.0\n",
       "10         -1    -1.0\n",
       "11          1     1.0\n",
       "12          1     1.0\n",
       "13         -1    -1.0\n",
       "14         -1    -1.0\n",
       "15          1     1.0\n",
       "16         -1    -1.0\n",
       "17          1     1.0\n",
       "18         -1    -1.0\n",
       "19          1     0.0\n",
       "20          1     1.0\n",
       "21          1     1.0\n",
       "22          1    -1.0\n",
       "23          1     1.0\n",
       "24          1     1.0\n",
       "25          1     1.0\n",
       "26         -1    -1.0\n",
       "27         -1    -1.0\n",
       "28         -1    -1.0\n",
       "29         -1    -1.0\n",
       "..        ...     ...\n",
       "70          1     1.0\n",
       "71          1     1.0\n",
       "72         -1     0.0\n",
       "73         -1    -1.0\n",
       "74         -1    -1.0\n",
       "75         -1    -1.0\n",
       "76          1     1.0\n",
       "77          1    -1.0\n",
       "78          1     1.0\n",
       "79          1     1.0\n",
       "80          1    -1.0\n",
       "81          1     1.0\n",
       "82          1    -1.0\n",
       "83          1     1.0\n",
       "84          1    -1.0\n",
       "85          1    -1.0\n",
       "86          1     1.0\n",
       "87          1    -1.0\n",
       "88          1     1.0\n",
       "89         -1    -1.0\n",
       "90         -1    -1.0\n",
       "91          1     1.0\n",
       "92          1     1.0\n",
       "93          1     1.0\n",
       "94          1     1.0\n",
       "95          1     1.0\n",
       "96          1     1.0\n",
       "97         -1    -1.0\n",
       "98          1     1.0\n",
       "99          1     1.0\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.45121412803532007"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yhat = rnn.predict( \n",
    "    [\n",
    "        #X_test_timestamp,\n",
    "        X_test_volume,\n",
    "        X_test_trends,\n",
    "        X_test_lagged_price\n",
    "    ],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "display(yhat)\n",
    "\n",
    "inverted_yhat = np.argmax(yhat,axis=1) #returns INDICES of max \n",
    "onehot_to_val_dict = {0: 0, 1: 1, 2:-1 }\n",
    "\n",
    "inverted_yhat_arr = np.asarray(inverted_yhat)\n",
    "predicted = [onehot_to_val_dict[i] for i in inverted_yhat_arr]\n",
    "\n",
    "\n",
    "df_pred_output = pd.DataFrame(predicted, columns=['predicted'])\n",
    "df_pred_output['actual'] = Y_test_is_spike\n",
    "#df_pred_output['index_output'] = inverted_yhat\n",
    "display(df_pred_output.head(100))\n",
    "\n",
    "correct = (df_pred_output['actual'].values == df_pred_output['predicted'].values)\n",
    "accuracy = correct.sum() / correct.size\n",
    "display(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON# serial \n",
    "model_json = rnn.to_json()\n",
    "with open(\"model_classification.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "rnn.save_weights(\"model_classification.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>-1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1.0</th>\n",
       "      <td>808</td>\n",
       "      <td>298</td>\n",
       "      <td>628</td>\n",
       "      <td>1734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>333</td>\n",
       "      <td>476</td>\n",
       "      <td>382</td>\n",
       "      <td>1191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>500</td>\n",
       "      <td>345</td>\n",
       "      <td>760</td>\n",
       "      <td>1605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1641</td>\n",
       "      <td>1119</td>\n",
       "      <td>1770</td>\n",
       "      <td>4530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    -1     0     1   All\n",
       "Actual                           \n",
       "-1.0        808   298   628  1734\n",
       "0.0         333   476   382  1191\n",
       "1.0         500   345   760  1605\n",
       "All        1641  1119  1770  4530"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "#print(metrics.confusion_matrix(df_pred_output['actual'].values, df_pred_output['predicted'].values,labels=[0,1,-1]))\n",
    "\n",
    "confusion_matrix = pd.crosstab(df_pred_output['actual'].values, df_pred_output['predicted'].values, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "\n",
    "display(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to check what the rnn actually learned \n",
    "# visualize predicted vs actual to get insight into this \n",
    "\n",
    "# try with instead of just 10% biggest changes, maybe with 25% \n",
    "# is it just learning from the previous prices, or is google trends actually helping \n",
    "# -> run rnn without google trends \n",
    "\n",
    "\n",
    "# I have a master_df_v2 now so try that - this one has 0.3 as cutoff for is Spike \n",
    "# Have to eventually get validation data - also get overall newer more data "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
