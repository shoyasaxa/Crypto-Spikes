{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras import Input\n",
    "from keras.engine import Model\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Concatenate, concatenate\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# features is a list of strings of feature names \n",
    "\n",
    "def build_model(features, data_length):\n",
    "    \n",
    "    inputs_list = [] \n",
    "    for feature_name in features:\n",
    "        inputs_list.append((Input(shape=(data_length,1), name=feature_name)))\n",
    "    \n",
    "    layers = [] \n",
    "    for i, input_name in enumerate(inputs_list): \n",
    "        layers.append(LSTM(64, return_sequences=False)(inputs_list[i]) )\n",
    "        \n",
    "    output = concatenate(layers) \n",
    "    output = Dense(3, activation='softmax', name='IsSpike')(output)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs = inputs_list,\n",
    "        outputs = [output]\n",
    "    )\n",
    "    \n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    \n",
    "    return model    \n",
    "\n",
    "data_length = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shoya\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Shoya\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Shoya\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Volume_BTC</th>\n",
       "      <th>Bitcoin_Adj</th>\n",
       "      <th>Close</th>\n",
       "      <th>Price_lagged</th>\n",
       "      <th>Is Spike</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.776791</td>\n",
       "      <td>0.471970</td>\n",
       "      <td>0.484557</td>\n",
       "      <td>0.484557</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.463316</td>\n",
       "      <td>0.439996</td>\n",
       "      <td>0.538331</td>\n",
       "      <td>0.538331</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.725079</td>\n",
       "      <td>0.529463</td>\n",
       "      <td>0.520715</td>\n",
       "      <td>0.520715</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.210661</td>\n",
       "      <td>0.416611</td>\n",
       "      <td>0.566098</td>\n",
       "      <td>0.566098</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.594148</td>\n",
       "      <td>0.445509</td>\n",
       "      <td>0.568881</td>\n",
       "      <td>0.568881</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Volume_BTC  Bitcoin_Adj     Close  Price_lagged  Is Spike\n",
       "1    0.776791     0.471970  0.484557      0.484557       1.0\n",
       "2    0.463316     0.439996  0.538331      0.538331       1.0\n",
       "3    0.725079     0.529463  0.520715      0.520715      -1.0\n",
       "4    0.210661     0.416611  0.566098      0.566098       0.0\n",
       "5    0.594148     0.445509  0.568881      0.568881      -1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Volume_BTC</th>\n",
       "      <th>Bitcoin_Adj</th>\n",
       "      <th>Close</th>\n",
       "      <th>Price_lagged</th>\n",
       "      <th>Is Spike</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30285</th>\n",
       "      <td>0.455905</td>\n",
       "      <td>0.449846</td>\n",
       "      <td>0.546519</td>\n",
       "      <td>0.546519</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30286</th>\n",
       "      <td>0.308749</td>\n",
       "      <td>0.457405</td>\n",
       "      <td>0.549938</td>\n",
       "      <td>0.549938</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30287</th>\n",
       "      <td>0.632915</td>\n",
       "      <td>0.453625</td>\n",
       "      <td>0.563428</td>\n",
       "      <td>0.563428</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30288</th>\n",
       "      <td>0.629822</td>\n",
       "      <td>0.434124</td>\n",
       "      <td>0.506657</td>\n",
       "      <td>0.506657</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30289</th>\n",
       "      <td>0.564642</td>\n",
       "      <td>0.445374</td>\n",
       "      <td>0.493535</td>\n",
       "      <td>0.493535</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Volume_BTC  Bitcoin_Adj     Close  Price_lagged  Is Spike\n",
       "30285    0.455905     0.449846  0.546519      0.546519       1.0\n",
       "30286    0.308749     0.457405  0.549938      0.549938      -1.0\n",
       "30287    0.632915     0.453625  0.563428      0.563428       0.0\n",
       "30288    0.629822     0.434124  0.506657      0.506657       0.0\n",
       "30289    0.564642     0.445374  0.493535      0.493535      -1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "master_df = pd.read_csv('C:/Users/Shoya/surf/data/master_df_v3.csv', encoding='latin1')\n",
    "df = master_df[['Timestamp', 'Close', 'Volume_(BTC)', 'Volume_(Currency)', 'Date(UTC)', 'Bitcoin (Adj.Overlap)', \n",
    "               'Close Price % Change', 'Close Price % Change (Abs)', 'Is Spike']]\n",
    "\n",
    "# lag inputs depending on data_length \n",
    "df['Price_lagged'] = df['Close']#.shift(data_length)\n",
    "df['Volume_BTC'] = df['Volume_(BTC)']#.shift(data_length)\n",
    "df['Bitcoin_Adj'] = df['Bitcoin (Adj.Overlap)']#.shift(data_length)\n",
    "\n",
    "df = df.dropna()\n",
    "cols = ['Volume_BTC','Bitcoin_Adj', 'Close', 'Price_lagged']\n",
    "\n",
    "# Stationalize Data by taking log differences\n",
    "data_array = np.diff(np.log(df[cols]), axis=0)\n",
    "\n",
    "# Min-Max Scale \n",
    "\n",
    "scalers = {}\n",
    "datas = [] \n",
    "\n",
    "df_scaled = pd.DataFrame(columns=cols)\n",
    "\n",
    "\n",
    "############################################################\n",
    "#  Fix below - I am scaling the whole data set together, when I should scale the train and test datasets separately\n",
    "############################################################\n",
    "\n",
    "for i in range(len(cols)): \n",
    "    scalers[cols[i]] = MinMaxScaler()\n",
    "    #print('data', data_array[:,i])\n",
    "    \n",
    "    col_data = data_array[:,i]\n",
    "    col_data = np.reshape(col_data, (len(col_data), 1))\n",
    "    \n",
    "    data = scalers[cols[i]].fit_transform( col_data )\n",
    "    #print('scaled', data)\n",
    "    data = np.reshape(data, (1, len(data)))\n",
    "    df_scaled[cols[i]] = data[0]\n",
    "    \n",
    "df_scaled['Is Spike'] = df['Is Spike']\n",
    "df_scaled.dropna(inplace=True)\n",
    "display(df_scaled.head())\n",
    "display(df_scaled.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  0.,  0., ...,  1.,  1., -1.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# split and reshape data to feed into RNN\n",
    "\n",
    "# X_timestamp = df_scaled['Timestamp'].values\n",
    "X_volume = df_scaled['Volume_BTC'].values\n",
    "X_trends = df_scaled['Bitcoin_Adj'].values\n",
    "X_lagged_price = df_scaled['Price_lagged'].values\n",
    "\n",
    "Y_is_spike = df_scaled['Is Spike'].values \n",
    "\n",
    "train_size = int(len(X_volume) * 0.85)\n",
    "train_size = int(train_size/data_length) * data_length\n",
    "\n",
    "test_size_index = int(len(X_volume)/data_length)*data_length\n",
    "\n",
    "X_train_volume = []\n",
    "X_test_volume = [] \n",
    "X_train_trends = []\n",
    "X_test_trends = []\n",
    "X_train_lagged_price = []\n",
    "X_test_lagged_price = []\n",
    "Y_train_is_spike = [] \n",
    "Y_test_is_spike = [] \n",
    "\n",
    "for i in range(train_size-data_length):\n",
    "    vol_temp = []\n",
    "    trends_temp = []\n",
    "    price_temp = []\n",
    "    for j in range(data_length):\n",
    "        vol_temp.append(X_volume[i+j])\n",
    "        trends_temp.append(X_trends[i+j])\n",
    "        price_temp.append(X_lagged_price[i+j])\n",
    "    X_train_volume.append(vol_temp)\n",
    "    X_train_trends.append(trends_temp)\n",
    "    X_train_lagged_price.append(price_temp)\n",
    "    \n",
    "    Y_train_is_spike.append(Y_is_spike[i+data_length])\n",
    "\n",
    "for i in range(test_size_index-train_size-data_length):\n",
    "    vol_temp = []\n",
    "    trends_temp = [] \n",
    "    price_temp = [] \n",
    "    for j in range(data_length):\n",
    "        vol_temp.append(X_volume[train_size+i+j])\n",
    "        trends_temp.append(X_trends[train_size+i+j])\n",
    "        price_temp.append(X_lagged_price[train_size+i+j])\n",
    "    X_test_volume.append(vol_temp)\n",
    "    X_test_trends.append(trends_temp)\n",
    "    X_test_lagged_price.append(price_temp)\n",
    "    \n",
    "    Y_test_is_spike.append(Y_is_spike[train_size+i+data_length])\n",
    "    \n",
    "X_train_volume = np.array(X_train_volume)\n",
    "X_test_volume =  np.array(X_test_volume)\n",
    "X_train_trends = np.array(X_train_trends)\n",
    "X_test_trends = np.array(X_test_trends)\n",
    "X_train_lagged_price = np.array(X_train_lagged_price)\n",
    "X_test_lagged_price = np.array(X_test_lagged_price)\n",
    "Y_train_is_spike =  np.array(Y_train_is_spike)\n",
    "Y_test_is_spike = np.array(Y_test_is_spike)\n",
    "    \n",
    "    \n",
    "Y_train_is_spike_onehot = to_categorical(Y_train_is_spike, num_classes=3)\n",
    "Y_test_is_spike_onehot = to_categorical(Y_test_is_spike,num_classes=3)\n",
    "display(Y_train_is_spike)\n",
    "\n",
    "# y = pd.DataFrame(Y_train_is_spike_onehot)\n",
    "# y['actual'] = Y_train_is_spike\n",
    "# display(y.head(25))\n",
    "    \n",
    "# display(X_train_trends.shape)\n",
    "# display(Y_train_is_spike.shape)\n",
    "\n",
    "#display(X_train_lagged_price)\n",
    "#display(Y_train_is_spike)\n",
    "\n",
    "# df_train = pd.DataFrame(X_train_lagged_price)\n",
    "# df_train['label'] = Y_train_is_spike\n",
    "# display(df_train.tail(20))\n",
    "# display(df_scaled.head(30))\n",
    "# display(df_train.head(30))\n",
    "\n",
    "#--------------------------------\n",
    "\n",
    "# # X_train_timestamp, X_test_timestamp = X_timestamp[:train_size], X_timestamp[train_size:test_size_index ]\n",
    "# X_train_volume, X_test_volume = X_volume[:train_size], X_volume[train_size:test_size_index ]\n",
    "# X_train_trends, X_test_trends = X_trends[:train_size], X_trends[train_size:test_size_index ]\n",
    "# X_train_lagged_price, X_test_lagged_price = X_lagged_price[:train_size], X_lagged_price[train_size:test_size_index ]\n",
    "\n",
    "# # becasue I lagged the x inputs, I should forward the Y's by the data_length as well \n",
    "# Y_train_is_spike, Y_test_is_spike = Y_is_spike[data_length:train_size], Y_is_spike[train_size+data_length:test_size_index ]\n",
    "\n",
    "\n",
    "# # X.shape is (samples, timesteps, dimension) \n",
    "# # timestemps is 15, samples is just however many nobs there are (but it doesn't matter, so it should be None)\n",
    "\n",
    "\n",
    "X_train_volume = np.reshape(X_train_volume, (X_train_volume.shape[0],data_length,1) ) \n",
    "X_train_trends = np.reshape(X_train_trends, (X_train_trends.shape[0],data_length,1) ) \n",
    "X_train_lagged_price = np.reshape(X_train_lagged_price, (X_train_lagged_price.shape[0], data_length, 1))\n",
    "\n",
    "X_test_volume = np.reshape(X_test_volume, (X_test_volume.shape[0],data_length,1) ) \n",
    "X_test_trends = np.reshape(X_test_trends, (X_test_trends.shape[0],data_length,1) )  \n",
    "X_test_lagged_price = np.reshape(X_test_lagged_price, (X_test_lagged_price.shape[0],data_length,1))\n",
    "\n",
    "\n",
    "# # X_train_timestamp = np.reshape(X_train_timestamp, (int(X_train_timestamp.shape[0]/data_length),data_length,1) ) \n",
    "# X_train_volume = np.reshape(X_train_volume, (int(X_train_volume.shape[0]/data_length),data_length,1) ) \n",
    "# X_train_trends = np.reshape(X_train_trends, (int(X_train_trends.shape[0]/data_length),data_length,1) ) \n",
    "# X_train_lagged_price = np.reshape(X_train_lagged_price, (int(X_train_lagged_price.shape[0]/data_length), data_length, 1))\n",
    "\n",
    "# # X_test_timestamp = np.reshape(X_test_timestamp, (int(X_test_timestamp.shape[0]/data_length),data_length,1) ) \n",
    "# X_test_volume = np.reshape(X_test_volume, (int(X_test_volume.shape[0]/data_length),data_length,1) ) \n",
    "# X_test_trends = np.reshape(X_test_trends, (int(X_test_trends.shape[0]/data_length),data_length,1) )  \n",
    "# X_test_lagged_price = np.reshape(X_test_lagged_price, (int(X_test_lagged_price.shape[0]/data_length),data_length,1))\n",
    "\n",
    "\n",
    "# # Don't need the 1 for the third dimension for Y's??\n",
    "\n",
    "\n",
    "# Y_train_is_spike = np.reshape(Y_train_is_spike, (int(Y_train_is_spike.shape[0]/data_length),  data_length) ) \n",
    "# Y_test_is_spike = np.reshape(Y_test_is_spike, (int(Y_test_is_spike.shape[0]/data_length),  data_length) )\n",
    "\n",
    "#-----------------------------------\n",
    "\n",
    "\n",
    "# instead of using input 1,2,3,4,5,6,7,8,9,10 to predict output for 11,12,13,14,15,16,17,18,19,20\n",
    "# I want to use input 1,2,3,4,5,6,7,8,9,10 to predict output for 11, then 2,3,4,5,6,7,8,9,10,11 to predict output for 12 \n",
    "\n",
    "# right now I am actually feeding input 1,2,3,4,5,6,7,8,9,10 to predict output for 1,2,3,4,5,6,7,8,9,10. \n",
    "# instead I should at least feed 1,2,3..8,9,10 to predict 11,12,13,14,15,16,17,18,19,20 -> lag everything by data_length! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "25730/25730 [==============================] - 13s - loss: 1.0821 - categorical_accuracy: 0.4243    \n",
      "Epoch 2/200\n",
      "25730/25730 [==============================] - 13s - loss: 1.0812 - categorical_accuracy: 0.4243    \n",
      "Epoch 3/200\n",
      "25730/25730 [==============================] - 13s - loss: 1.0801 - categorical_accuracy: 0.4243    \n",
      "Epoch 4/200\n",
      "25730/25730 [==============================] - 13s - loss: 1.0763 - categorical_accuracy: 0.4253    \n",
      "Epoch 5/200\n",
      "25730/25730 [==============================] - 13s - loss: 1.0701 - categorical_accuracy: 0.4351     ETA: 4s - loss: 1.0699 -  - ETA: 3s - l\n",
      "Epoch 6/200\n",
      "25730/25730 [==============================] - 13s - loss: 1.0638 - categorical_accuracy: 0.4488    \n",
      "Epoch 7/200\n",
      "25730/25730 [==============================] - 13s - loss: 1.0570 - categorical_accuracy: 0.4575    \n",
      "Epoch 8/200\n",
      "25730/25730 [==============================] - 14s - loss: 1.0453 - categorical_accuracy: 0.4743    \n",
      "Epoch 9/200\n",
      "25730/25730 [==============================] - 13s - loss: 1.0346 - categorical_accuracy: 0.4855    \n",
      "Epoch 10/200\n",
      "25730/25730 [==============================] - 13s - loss: 1.0213 - categorical_accuracy: 0.4989    \n",
      "Epoch 11/200\n",
      "25730/25730 [==============================] - 13s - loss: 1.0134 - categorical_accuracy: 0.5043    \n",
      "Epoch 12/200\n",
      "25730/25730 [==============================] - 13s - loss: 1.0039 - categorical_accuracy: 0.5137    \n",
      "Epoch 13/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.9967 - categorical_accuracy: 0.5211    \n",
      "Epoch 14/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.9914 - categorical_accuracy: 0.5261    \n",
      "Epoch 15/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.9909 - categorical_accuracy: 0.5260    \n",
      "Epoch 16/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.9836 - categorical_accuracy: 0.5325    \n",
      "Epoch 17/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.9802 - categorical_accuracy: 0.5354    \n",
      "Epoch 18/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.9778 - categorical_accuracy: 0.5353    \n",
      "Epoch 19/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.9707 - categorical_accuracy: 0.5404    \n",
      "Epoch 20/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.9696 - categorical_accuracy: 0.5401    \n",
      "Epoch 21/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.9635 - categorical_accuracy: 0.5455    \n",
      "Epoch 22/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.9627 - categorical_accuracy: 0.5477    \n",
      "Epoch 23/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.9581 - categorical_accuracy: 0.5507    \n",
      "Epoch 24/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.9538 - categorical_accuracy: 0.5545    \n",
      "Epoch 25/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.9525 - categorical_accuracy: 0.5537    \n",
      "Epoch 26/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.9506 - categorical_accuracy: 0.5557    \n",
      "Epoch 27/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.9490 - categorical_accuracy: 0.5580     ETA: 0s - loss: 0.9485 - catego\n",
      "Epoch 28/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.9445 - categorical_accuracy: 0.5613    \n",
      "Epoch 29/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.9405 - categorical_accuracy: 0.5649    \n",
      "Epoch 30/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.9356 - categorical_accuracy: 0.5709    \n",
      "Epoch 31/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.9332 - categorical_accuracy: 0.5670    \n",
      "Epoch 32/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.9287 - categorical_accuracy: 0.5730    \n",
      "Epoch 33/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.9222 - categorical_accuracy: 0.5772    \n",
      "Epoch 34/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.9153 - categorical_accuracy: 0.5819    \n",
      "Epoch 35/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.9099 - categorical_accuracy: 0.5893    \n",
      "Epoch 36/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.8942 - categorical_accuracy: 0.6007    \n",
      "Epoch 37/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.8835 - categorical_accuracy: 0.6076    \n",
      "Epoch 38/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.8751 - categorical_accuracy: 0.6138    \n",
      "Epoch 39/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.8692 - categorical_accuracy: 0.6202    \n",
      "Epoch 40/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.8621 - categorical_accuracy: 0.6236    \n",
      "Epoch 41/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.8558 - categorical_accuracy: 0.6269    \n",
      "Epoch 42/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.8528 - categorical_accuracy: 0.6270    \n",
      "Epoch 43/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.8472 - categorical_accuracy: 0.6319    \n",
      "Epoch 44/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.8436 - categorical_accuracy: 0.6344    \n",
      "Epoch 45/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.8396 - categorical_accuracy: 0.6393    \n",
      "Epoch 46/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.8320 - categorical_accuracy: 0.6434    \n",
      "Epoch 47/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.8335 - categorical_accuracy: 0.6389    \n",
      "Epoch 48/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.8304 - categorical_accuracy: 0.6433    \n",
      "Epoch 49/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.8213 - categorical_accuracy: 0.6469    \n",
      "Epoch 50/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.8165 - categorical_accuracy: 0.6473    \n",
      "Epoch 51/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.8101 - categorical_accuracy: 0.6512    \n",
      "Epoch 52/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.8076 - categorical_accuracy: 0.6518    \n",
      "Epoch 53/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.7996 - categorical_accuracy: 0.6614    \n",
      "Epoch 54/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.8018 - categorical_accuracy: 0.6591    \n",
      "Epoch 55/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.7934 - categorical_accuracy: 0.6619    \n",
      "Epoch 56/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.7899 - categorical_accuracy: 0.6616    \n",
      "Epoch 57/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.7838 - categorical_accuracy: 0.6646    \n",
      "Epoch 58/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.7844 - categorical_accuracy: 0.6655    \n",
      "Epoch 59/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.7789 - categorical_accuracy: 0.6647    \n",
      "Epoch 60/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.7744 - categorical_accuracy: 0.6700    \n",
      "Epoch 61/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.7724 - categorical_accuracy: 0.6706    \n",
      "Epoch 62/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.7678 - categorical_accuracy: 0.6710    \n",
      "Epoch 63/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.7668 - categorical_accuracy: 0.6743    \n",
      "Epoch 64/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.7648 - categorical_accuracy: 0.6754    \n",
      "Epoch 65/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.7644 - categorical_accuracy: 0.6732    \n",
      "Epoch 66/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.7584 - categorical_accuracy: 0.6759     ETA: 0s - loss: 0.7577 - categorical_\n",
      "Epoch 67/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.7573 - categorical_accuracy: 0.6773    \n",
      "Epoch 68/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.7548 - categorical_accuracy: 0.6763    \n",
      "Epoch 69/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.7507 - categorical_accuracy: 0.6775    \n",
      "Epoch 70/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.7494 - categorical_accuracy: 0.6780    \n",
      "Epoch 71/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25730/25730 [==============================] - 13s - loss: 0.7476 - categorical_accuracy: 0.6808    \n",
      "Epoch 72/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.7475 - categorical_accuracy: 0.6801    \n",
      "Epoch 73/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.7422 - categorical_accuracy: 0.6824    \n",
      "Epoch 74/200\n",
      "25730/25730 [==============================] - 16s - loss: 0.7400 - categorical_accuracy: 0.6836    \n",
      "Epoch 75/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.7399 - categorical_accuracy: 0.6811    \n",
      "Epoch 76/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.7379 - categorical_accuracy: 0.6837    \n",
      "Epoch 77/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.7350 - categorical_accuracy: 0.6833    \n",
      "Epoch 78/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.7345 - categorical_accuracy: 0.6853    \n",
      "Epoch 79/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.7326 - categorical_accuracy: 0.6866    \n",
      "Epoch 80/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.7301 - categorical_accuracy: 0.6886    \n",
      "Epoch 81/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.7290 - categorical_accuracy: 0.6847    \n",
      "Epoch 82/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.7262 - categorical_accuracy: 0.6887    \n",
      "Epoch 83/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.7258 - categorical_accuracy: 0.6904    \n",
      "Epoch 84/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.7269 - categorical_accuracy: 0.6872    \n",
      "Epoch 85/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.7221 - categorical_accuracy: 0.6901    \n",
      "Epoch 86/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.7198 - categorical_accuracy: 0.6913    \n",
      "Epoch 87/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.7193 - categorical_accuracy: 0.6878    \n",
      "Epoch 88/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.7170 - categorical_accuracy: 0.6910    \n",
      "Epoch 89/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.7177 - categorical_accuracy: 0.6932    \n",
      "Epoch 90/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.7125 - categorical_accuracy: 0.6950    \n",
      "Epoch 91/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.7110 - categorical_accuracy: 0.6931    \n",
      "Epoch 92/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.7096 - categorical_accuracy: 0.6951    \n",
      "Epoch 93/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.7094 - categorical_accuracy: 0.6935    \n",
      "Epoch 94/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.7085 - categorical_accuracy: 0.6946    \n",
      "Epoch 95/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.7075 - categorical_accuracy: 0.6970    \n",
      "Epoch 96/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.7094 - categorical_accuracy: 0.6991    \n",
      "Epoch 97/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.7061 - categorical_accuracy: 0.6972    \n",
      "Epoch 98/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.7041 - categorical_accuracy: 0.6956    \n",
      "Epoch 99/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.7015 - categorical_accuracy: 0.6970    \n",
      "Epoch 100/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.7008 - categorical_accuracy: 0.6986    \n",
      "Epoch 101/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6988 - categorical_accuracy: 0.6975    \n",
      "Epoch 102/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6978 - categorical_accuracy: 0.6995    \n",
      "Epoch 103/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6978 - categorical_accuracy: 0.6981    \n",
      "Epoch 104/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6961 - categorical_accuracy: 0.6994    \n",
      "Epoch 105/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6960 - categorical_accuracy: 0.6973    \n",
      "Epoch 106/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6940 - categorical_accuracy: 0.6997    \n",
      "Epoch 107/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6895 - categorical_accuracy: 0.7025    \n",
      "Epoch 108/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6912 - categorical_accuracy: 0.7000    \n",
      "Epoch 109/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6908 - categorical_accuracy: 0.7000    \n",
      "Epoch 110/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6882 - categorical_accuracy: 0.7044    \n",
      "Epoch 111/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6920 - categorical_accuracy: 0.7002    \n",
      "Epoch 112/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6889 - categorical_accuracy: 0.7018    \n",
      "Epoch 113/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6864 - categorical_accuracy: 0.7056    \n",
      "Epoch 114/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6871 - categorical_accuracy: 0.7039    \n",
      "Epoch 115/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6838 - categorical_accuracy: 0.7048    \n",
      "Epoch 116/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6848 - categorical_accuracy: 0.7039    \n",
      "Epoch 117/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6803 - categorical_accuracy: 0.7074    \n",
      "Epoch 118/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6841 - categorical_accuracy: 0.7031    \n",
      "Epoch 119/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6808 - categorical_accuracy: 0.7035    \n",
      "Epoch 120/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6763 - categorical_accuracy: 0.7057    \n",
      "Epoch 121/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6785 - categorical_accuracy: 0.7066    \n",
      "Epoch 122/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6753 - categorical_accuracy: 0.7065    \n",
      "Epoch 123/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6761 - categorical_accuracy: 0.7075    \n",
      "Epoch 124/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6746 - categorical_accuracy: 0.7075    \n",
      "Epoch 125/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6736 - categorical_accuracy: 0.7108    \n",
      "Epoch 126/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6745 - categorical_accuracy: 0.7086    \n",
      "Epoch 127/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6746 - categorical_accuracy: 0.7055    \n",
      "Epoch 128/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6720 - categorical_accuracy: 0.7075    \n",
      "Epoch 129/200\n",
      "25730/25730 [==============================] - 16s - loss: 0.6709 - categorical_accuracy: 0.7079    \n",
      "Epoch 130/200\n",
      "25730/25730 [==============================] - 17s - loss: 0.6679 - categorical_accuracy: 0.7127    \n",
      "Epoch 131/200\n",
      "25730/25730 [==============================] - 16s - loss: 0.6698 - categorical_accuracy: 0.7083    \n",
      "Epoch 132/200\n",
      "25730/25730 [==============================] - 14s - loss: 0.6680 - categorical_accuracy: 0.7116    \n",
      "Epoch 133/200\n",
      "25730/25730 [==============================] - 15s - loss: 0.6656 - categorical_accuracy: 0.7126    \n",
      "Epoch 134/200\n",
      "25730/25730 [==============================] - 13s - loss: 0.6634 - categorical_accuracy: 0.7110    \n",
      "Epoch 135/200\n",
      "25730/25730 [==============================] - 15s - loss: 0.6668 - categorical_accuracy: 0.7124    \n",
      "Epoch 136/200\n",
      "25730/25730 [==============================] - 12s - loss: 0.6650 - categorical_accuracy: 0.7129    \n",
      "Epoch 137/200\n",
      " 4288/25730 [===>..........................] - ETA: 10s - loss: 0.6771 - categorical_accuracy: 0.7059"
     ]
    }
   ],
   "source": [
    "#features = ['Volume_BTC', 'Bitcoin_Adj', 'Price_lagged']\n",
    "features = ['Volume_BTC', 'Price_lagged']\n",
    "\n",
    "rnn = build_model(features, 10) \n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "history = rnn.fit(\n",
    "    [\n",
    "        #X_train_timestamp,\n",
    "        X_train_volume,\n",
    "        #X_train_trends,\n",
    "        X_train_lagged_price\n",
    "    ],\n",
    "    [\n",
    "        Y_train_is_spike_onehot\n",
    "    ]\n",
    "    ,\n",
    "#     validation_data=(\n",
    "#         [\n",
    "#             #X_test_timestamp,\n",
    "#             X_test_volume,\n",
    "#             #X_test_trends,\n",
    "#             X_test_lagged_price\n",
    "#         ],\n",
    "#         [\n",
    "#             Y_test_is_spike_onehot\n",
    "#         ]),\n",
    "    epochs=200,\n",
    "    batch_size=64,\n",
    "    callbacks=[\n",
    "      tensorboard_callback\n",
    "    ],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEgQUeiLiASR/w+OKfTGo1k80gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cd52be4a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "#plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4448/4530 [============================>.] - ETA: 0s\n",
      "\n",
      "Accuracy: 43.11%\n"
     ]
    }
   ],
   "source": [
    "score = rnn.evaluate(\n",
    "    [\n",
    "        #X_test_timestamp,\n",
    "        X_test_volume,\n",
    "        #X_test_trends,\n",
    "        X_test_lagged_price\n",
    "    ],\n",
    "    [\n",
    "        Y_test_is_spike_onehot\n",
    "    ])\n",
    "\n",
    "print('\\n')\n",
    "print(\"Accuracy: %.2f%%\" % (score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To test if Google Trends actually have any benefit in predicting spikes, I ran one with and without the trend data as input. \n",
    "\n",
    "#### For \"Is Spike\" cutoff of 0.1, (meaning Is Spike marks only the 10% biggest changes)\n",
    "    * With trend data, accuracy was 78.90% on test data.\n",
    "    * Without trend data, accuracy was 82.93% on test data.\n",
    "    \n",
    "#### For \"Is Spike\" cutoff of 0.3, \n",
    "    * With trend data, accuracy was 84.57% for epoch=40 and 89.69% for epoch=60, and 87.98% for epoch=100\n",
    "    * Without trend data, accuracy was 78.40% for epoch=40 and 88.88% for epoch=60, and 93.60% for epoch=100\n",
    "    \n",
    "    Thus Google Trends actually helped Is Spike in the latter case!!!! \n",
    "    \n",
    "    \n",
    "    * Accuracy on test data is much better than that of train data \n",
    "        * -> could be because the test data is statistically different than train data \n",
    "            * which makese sense because test data is the real big spike \n",
    "                    * Since I am using 10 hours of data to predict the next hour, it would make sense that the accuracy is good during this time since this is the time that people were looking up Bitcoin and perhaps buying them a few hours later \n",
    "                * get more up-to-date data \n",
    "        * OR BECAUSE TRAIN AND TEST DATA SOMEHOW OVERLAPS?\n",
    "        \n",
    "#### With updated data\n",
    "    * With trend data, accuracy was 45.12% for epoch=100, 45.56% for epoch=200 -> overfitting?\n",
    "        * -> put in dropout \n",
    "    - Without trend data, accuracy was 43.11% for epoch=100, \n",
    "    \n",
    "    * with the updated data, the test data is now \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02534344,  0.05034934,  0.92430729],\n",
       "       [ 0.03313668,  0.90681744,  0.06004593],\n",
       "       [ 0.43258792,  0.43481657,  0.1325956 ],\n",
       "       ..., \n",
       "       [ 0.37551153,  0.06019156,  0.56429696],\n",
       "       [ 0.53921092,  0.44531742,  0.01547169],\n",
       "       [ 0.17624743,  0.30557993,  0.51817256]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    predicted  actual\n",
       "0          -1    -1.0\n",
       "1           1     1.0\n",
       "2           1     1.0\n",
       "3          -1    -1.0\n",
       "4           1     1.0\n",
       "5           1     1.0\n",
       "6          -1    -1.0\n",
       "7          -1    -1.0\n",
       "8          -1    -1.0\n",
       "9          -1    -1.0\n",
       "10         -1    -1.0\n",
       "11          1     1.0\n",
       "12          1     1.0\n",
       "13         -1    -1.0\n",
       "14         -1    -1.0\n",
       "15          1     1.0\n",
       "16         -1    -1.0\n",
       "17          1     1.0\n",
       "18         -1    -1.0\n",
       "19          1     0.0\n",
       "20          1     1.0\n",
       "21          1     1.0\n",
       "22          1    -1.0\n",
       "23          1     1.0\n",
       "24          1     1.0\n",
       "25          1     1.0\n",
       "26         -1    -1.0\n",
       "27         -1    -1.0\n",
       "28         -1    -1.0\n",
       "29         -1    -1.0\n",
       "..        ...     ...\n",
       "70          1     1.0\n",
       "71          1     1.0\n",
       "72         -1     0.0\n",
       "73         -1    -1.0\n",
       "74         -1    -1.0\n",
       "75         -1    -1.0\n",
       "76          1     1.0\n",
       "77          1    -1.0\n",
       "78          1     1.0\n",
       "79          1     1.0\n",
       "80          1    -1.0\n",
       "81          1     1.0\n",
       "82          1    -1.0\n",
       "83          1     1.0\n",
       "84          1    -1.0\n",
       "85          1    -1.0\n",
       "86          1     1.0\n",
       "87          1    -1.0\n",
       "88          1     1.0\n",
       "89         -1    -1.0\n",
       "90         -1    -1.0\n",
       "91          1     1.0\n",
       "92          1     1.0\n",
       "93          1     1.0\n",
       "94          1     1.0\n",
       "95          1     1.0\n",
       "96          1     1.0\n",
       "97         -1    -1.0\n",
       "98          1     1.0\n",
       "99          1     1.0\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.45121412803532007"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yhat = rnn.predict( \n",
    "    [\n",
    "        #X_test_timestamp,\n",
    "        X_test_volume,\n",
    "        X_test_trends,\n",
    "        X_test_lagged_price\n",
    "    ],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "display(yhat)\n",
    "\n",
    "inverted_yhat = np.argmax(yhat,axis=1) #returns INDICES of max \n",
    "onehot_to_val_dict = {0: 0, 1: 1, 2:-1 }\n",
    "\n",
    "inverted_yhat_arr = np.asarray(inverted_yhat)\n",
    "predicted = [onehot_to_val_dict[i] for i in inverted_yhat_arr]\n",
    "\n",
    "\n",
    "df_pred_output = pd.DataFrame(predicted, columns=['predicted'])\n",
    "df_pred_output['actual'] = Y_test_is_spike\n",
    "#df_pred_output['index_output'] = inverted_yhat\n",
    "display(df_pred_output.head(100))\n",
    "\n",
    "correct = (df_pred_output['actual'].values == df_pred_output['predicted'].values)\n",
    "accuracy = correct.sum() / correct.size\n",
    "display(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON# serial \n",
    "model_json = rnn.to_json()\n",
    "with open(\"model_classification.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "rnn.save_weights(\"model_classification.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>-1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1.0</th>\n",
       "      <td>808</td>\n",
       "      <td>298</td>\n",
       "      <td>628</td>\n",
       "      <td>1734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>333</td>\n",
       "      <td>476</td>\n",
       "      <td>382</td>\n",
       "      <td>1191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>500</td>\n",
       "      <td>345</td>\n",
       "      <td>760</td>\n",
       "      <td>1605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1641</td>\n",
       "      <td>1119</td>\n",
       "      <td>1770</td>\n",
       "      <td>4530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    -1     0     1   All\n",
       "Actual                           \n",
       "-1.0        808   298   628  1734\n",
       "0.0         333   476   382  1191\n",
       "1.0         500   345   760  1605\n",
       "All        1641  1119  1770  4530"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "#print(metrics.confusion_matrix(df_pred_output['actual'].values, df_pred_output['predicted'].values,labels=[0,1,-1]))\n",
    "\n",
    "confusion_matrix = pd.crosstab(df_pred_output['actual'].values, df_pred_output['predicted'].values, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "\n",
    "display(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to check what the rnn actually learned \n",
    "# visualize predicted vs actual to get insight into this \n",
    "\n",
    "# try with instead of just 10% biggest changes, maybe with 25% \n",
    "# is it just learning from the previous prices, or is google trends actually helping \n",
    "# -> run rnn without google trends \n",
    "\n",
    "\n",
    "# I have a master_df_v2 now so try that - this one has 0.3 as cutoff for is Spike \n",
    "# Have to eventually get validation data - also get overall newer more data "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
